<template>
  <div class="mobile-practice-a">
    <!-- Compact Mobile Header -->
    <header class="mobile-header">
      <button class="header-btn" @click="toggleRecordingSets">
        <span class="icon">📚</span>
      </button>
      <div class="header-title">
        <h1>{{ currentRecording?.name || 'Practice' }}</h1>
        <span class="progress" v-if="activeSet">{{ activeSet.name }}</span>
      </div>
      <button class="header-btn" @click="showOptionsMenu">
        <span class="icon">⋮</span>
      </button>
    </header>

    <!-- Compact Audio Info -->
    <section class="audio-info-compact">
      <div class="audio-name">{{ getCurrentAudioName() }}</div>
      <div class="audio-controls">
        <button class="load-btn" @click="showLoadOptions">
          <span class="btn-icon">📁</span>
          <span class="btn-text">Load File</span>
        </button>
      </div>
    </section>

    <!-- Target Audio Visualizations -->
    <section class="audio-section target-section">
      <div class="section-label">Target Audio</div>
      <div class="waveform-container target-wave" ref="targetWaveformRef">
        <div v-if="!targetReady" class="placeholder">🎵 Target waveform will appear here</div>
      </div>
      <div class="spectrogram-container large target-spectrogram" ref="targetSpectrogramRef">
        <div v-if="!targetReady" class="placeholder large">Target SPECTROGRAM</div>
      </div>
    </section>

    <!-- User Audio Visualizations - only show when audio exists -->
    <section class="audio-section user-section" v-if="userAudioUrl">
      <div class="spectrogram-container large user-spectrogram" ref="userSpectrogramRef">
        <div v-if="!userReady" class="placeholder large">User SPECTROGRAM</div>
      </div>
      <div class="section-label">User Audio</div>
      <div class="waveform-container user-wave" ref="userWaveformRef">
        <div v-if="!userReady" class="placeholder">🎵 User waveform will appear here</div>
      </div>
    </section>

    <!-- Recording Placeholder when no user audio -->
    <section class="audio-section recording-section" v-else>
      <div class="recording-placeholder">
        <div class="record-button-large" @click="startRecording" :disabled="!getTargetAudioUrl() || recorderIsRecording">
          <span class="record-icon">{{ recorderIsRecording ? '⏹️' : '🎤' }}</span>
          <span class="record-text">{{ recorderIsRecording ? 'Recording...' : 'Tap to Record' }}</span>
        </div>
      </div>
    </section>

    <!-- Compact Controls -->
    <section class="controls-section">
      <!-- Playback Controls Row -->
      <div class="playback-row">
        <PlaybackButton 
          icon="▶️" 
          label="Play Target" 
          variant="target"
          :disabled="!getTargetAudioUrl()"
          @click="playTarget"
        />
        
        <PlaybackButton 
          icon="▶️" 
          label="Play User" 
          variant="user"
          :disabled="!userAudioUrl"
          @click="playUser"
        />
        
        <PlaybackButton 
          icon="▶️" 
          label="Play Both" 
          variant="overlapping"
          :disabled="!getTargetAudioUrl() || !userAudioUrl"
          @click="playBoth"
        />
        
        <PlaybackButton 
          icon="⚡" 
          :label="`Speed ${currentSpeed}x`" 
          variant="default"
          @click="cycleSpeed"
        />
      </div>
      
      <!-- Record Button Row -->
      <div class="record-row" v-if="userAudioUrl">
        <PlaybackButton 
          icon="🎤" 
          :label="recorderIsRecording ? 'Stop' : 'Record Again'" 
          variant="primary"
          :disabled="!getTargetAudioUrl() || isProcessing"
          :processing="isProcessing"
          @click="toggleRecording"
        />
      </div>
    </section>

    <!-- Bottom Navigation -->
    <nav class="bottom-navigation" v-if="activeSet">
      <div class="nav-center">
        <span class="nav-progress">{{ activeSet.name }}</span>
        <button v-if="hasUserRecording && currentRecording" class="complete-btn" @click="markRecordingCompleted">
          Mark Complete
        </button>
      </div>
    </nav>

    <!-- Bottom Safe Area -->
    <div class="bottom-safe-area"></div>

    <!-- Hidden File Input -->
    <input 
      type="file" 
      accept="audio/*" 
      @change="handleFileSelection" 
      ref="hiddenFileInput"
      style="display: none;"
    />

    <!-- URL Input Modal -->
    <div v-if="showUrlModal" class="modal-overlay" @click="showUrlModal = false">
      <div class="modal-content" @click.stop>
        <div class="modal-header">
          <h3>🌐 Load Audio from URL</h3>
          <button class="close-btn" @click="showUrlModal = false">×</button>
        </div>
        <div class="modal-body">
          <input 
            type="url" 
            v-model="tempAudioUrl" 
            placeholder="Enter audio URL (e.g., https://example.com/audio.mp3)"
            class="url-input"
            @keyup.enter="handleUrlLoad"
            ref="urlInputRef"
          />
          <div class="modal-actions">
            <button @click="showUrlModal = false" class="cancel-btn">Cancel</button>
            <button @click="handleUrlLoad" :disabled="!tempAudioUrl.trim()" class="load-btn-modal">
              Load Audio
            </button>
          </div>
        </div>
      </div>
    </div>

    <!-- Recording Sets Modal -->
    <div v-if="showRecordingSetsModal" class="modal-overlay" @click="showRecordingSetsModal = false">
      <div class="modal-content" @click.stop>
        <div class="modal-header">
          <h3>Recording Sets</h3>
          <button class="close-btn" @click="showRecordingSetsModal = false">×</button>
        </div>
        <div class="modal-body">
          <div v-if="!activeSet" class="empty-state">
            <p>No recording set active</p>
            <p>Please use the desktop version to create and manage recording sets.</p>
          </div>
          <div v-else class="recording-sets-list">
            <div class="recording-set-item active">
              <div class="set-info">
                <h4>{{ activeSet.name }}</h4>
                <p>{{ activeSet.recordings?.length || 0 }} recordings</p>
                <div class="progress-bar">
                  <div class="progress-fill" :style="{ width: (activeSet.progress?.percentage || 0) + '%' }"></div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Options Modal -->
    <div v-if="showOptionsModal" class="modal-overlay" @click="showOptionsModal = false">
      <div class="modal-content" @click.stop>
        <div class="modal-header">
          <h3>Options</h3>
          <button class="close-btn" @click="showOptionsModal = false">×</button>
        </div>
        <div class="modal-body">
          <div class="option-item" @click="selectFile">
            <span class="option-icon">📁</span>
            <span class="option-text">Upload Audio File</span>
          </div>
          <div class="option-item" @click="showUrlInput">
            <span class="option-icon">🌐</span>
            <span class="option-text">Load from URL</span>
          </div>
          <div class="option-item" @click="showFolderUpload">
            <span class="option-icon">📂</span>
            <span class="option-text">Upload Folder</span>
          </div>
          <div class="option-item" @click="showSettings">
            <span class="option-icon">⚙️</span>
            <span class="option-text">Settings</span>
          </div>
        </div>
      </div>
    </div>
  </div>
</template>

<script setup>
import { ref, computed, watch, onMounted, nextTick } from 'vue'
import { useWaveform } from '../composables/useWaveform'
import { useTimeSync } from '../composables/useTimeSync'
import { useRecordingSets } from '../composables/useRecordingSets'
import { useSmartAudioAlignment } from '../composables/useSmartAudioAlignment'
import { useIndexedDB } from '../composables/useIndexedDB'
import { useAudioProcessing } from '../composables/useAudioProcessing'
import { useAudioRecorder } from '../composables/useAudioRecorder'
import PlaybackButton from '../components/PlaybackButton.vue'

// Same composables as desktop
const { initDB, addRecording, deleteRecording } = useIndexedDB()
const { 
  isProcessing: vadIsProcessing,
  vadReady,
  initVAD,
  processAudio,
  normalizeAudioSilence,
  alignTwoAudios
} = useSmartAudioAlignment()

// Recording sets integration (same as desktop)
const { 
  activeSet, 
  currentRecording, 
  updateUserRecording,
  markRecordingCompleted 
} = useRecordingSets()

// Reactive state
const showRecordingSetsModal = ref(false)
const showOptionsModal = ref(false)
const showUrlModal = ref(false)
const tempAudioUrl = ref('')
const hiddenFileInput = ref(null)
const urlInputRef = ref(null)
const currentAudioName = ref('patth.wav')
const targetAudio = ref(null)
const targetAudioUrl = ref(null)
const hasUserRecording = ref(false)
const isProcessing = ref(false)
const userAudioUrl = ref(null)
const userAudioBlob = ref(null)
const userDuration = ref('0:02')
const vadEnabled = ref(true)
const autoPlayEnabled = ref(true)
const overlayMode = ref(false)
const isVadProcessing = ref(false)     // Track VAD processing state
const originalUserRecording = ref(null) // Store original recording before VAD trimming
const trimmedUserRecording = ref(null)  // Store VAD-trimmed recording
const vadTrimInfo = ref(null)           // Store VAD trimming information
const originalTargetAudio = ref(null)   // Store original target audio before VAD trimming
const trimmedTargetAudio = ref(null)    // Store VAD-trimmed target audio
const targetVadTrimInfo = ref(null)     // Store target VAD trimming information
const speedOptions = [0.5, 0.75, 1.0]
const currentSpeedIndex = ref(2) // Start with 1x (index 2)
const currentSpeed = computed(() => speedOptions[currentSpeedIndex.value])

// Waveform refs
const targetWaveformRef = ref(null)
const targetSpectrogramRef = ref(null)
const userWaveformRef = ref(null)
const userSpectrogramRef = ref(null)

// Time synchronization for proportional scaling
const { 
  syncEnabled, 
  targetWidthPercent, 
  userWidthPercent, 
  setTargetDuration, 
  setUserDuration 
} = useTimeSync()

// Audio processing for alignment and VAD
const { 
  autoAlignRecordings,
  trimAudioWithVAD,
  detectSpeechBoundariesVAD,
  vadReady: vadProcessorReady,
  initVAD: initVADProcessor
} = useAudioProcessing()

// Audio recording functionality
const { 
  isRecording: recorderIsRecording,
  recordingTime: recorderRecordingTime,
  startRecording: startAudioRecording,
  stopRecording: stopAudioRecording
} = useAudioRecorder()

// Function to add silence padding to audio blob
const addSilencePadding = async (audioBlob, paddingSeconds, atStart = true) => {
  console.log(`🔇 Adding ${paddingSeconds.toFixed(3)}s of silence ${atStart ? 'at start' : 'at end'}`)
  
  try {
    const audioContext = new (window.AudioContext || window.webkitAudioContext)()
    
    // Decode the original audio
    const arrayBuffer = await audioBlob.arrayBuffer()
    const audioBuffer = await audioContext.decodeAudioData(arrayBuffer)
    
    // Calculate new buffer length
    const sampleRate = audioBuffer.sampleRate
    const paddingSamples = Math.floor(paddingSeconds * sampleRate)
    const newLength = audioBuffer.length + paddingSamples
    
    // Create new buffer with padding
    const newBuffer = audioContext.createBuffer(
      audioBuffer.numberOfChannels,
      newLength,
      sampleRate
    )
    
    // Copy original audio data with padding
    for (let channel = 0; channel < audioBuffer.numberOfChannels; channel++) {
      const originalData = audioBuffer.getChannelData(channel)
      const newData = newBuffer.getChannelData(channel)
      
      if (atStart) {
        // Add silence at start, then original audio
        newData.set(originalData, paddingSamples)
      } else {
        // Add original audio, then silence at end
        newData.set(originalData, 0)
      }
    }
    
    // Convert back to blob
    const length = newBuffer.length
    const numberOfChannels = newBuffer.numberOfChannels
    const arrayBuffer2 = new ArrayBuffer(44 + length * numberOfChannels * 2)
    const view = new DataView(arrayBuffer2)
    
    // WAV header
    const writeString = (offset, string) => {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i))
      }
    }
    
    writeString(0, 'RIFF')
    view.setUint32(4, 36 + length * numberOfChannels * 2, true)
    writeString(8, 'WAVE')
    writeString(12, 'fmt ')
    view.setUint32(16, 16, true)
    view.setUint16(20, 1, true)
    view.setUint16(22, numberOfChannels, true)
    view.setUint32(24, sampleRate, true)
    view.setUint32(28, sampleRate * numberOfChannels * 2, true)
    view.setUint16(32, numberOfChannels * 2, true)
    view.setUint16(34, 16, true)
    writeString(36, 'data')
    view.setUint32(40, length * numberOfChannels * 2, true)
    
    // Convert float samples to 16-bit PCM
    let offset = 44
    for (let i = 0; i < length; i++) {
      for (let channel = 0; channel < numberOfChannels; channel++) {
        const sample = Math.max(-1, Math.min(1, newBuffer.getChannelData(channel)[i]))
        view.setInt16(offset, sample * 0x7FFF, true)
        offset += 2
      }
    }
    
    const paddedBlob = new Blob([arrayBuffer2], { type: 'audio/wav' })
    console.log(`🔇 Successfully added ${paddingSeconds.toFixed(3)}s padding`)
    
    await audioContext.close()
    return paddedBlob
    
  } catch (error) {
    console.error('🔇 Error adding silence padding:', error)
    return audioBlob // Return original if padding fails
  }
}

// Function to align audio based on VAD trimming information
const alignAudioWithVADInfo = async (userBlob, userVadInfo, targetVadInfo) => {
  console.log('🎯↔️🎤 Aligning user audio with target audio based on VAD info')
  
  if (!userVadInfo || !targetVadInfo) {
    console.log('🎯↔️🎤 No VAD info available for alignment, using original audio')
    return userBlob
  }
  
  const targetTrimmedStart = targetVadInfo.trimmedStart || 0
  const userTrimmedStart = userVadInfo.trimmedStart || 0
  const targetNewDuration = targetVadInfo.newDuration || 0
  const userNewDuration = userVadInfo.newDuration || 0
  
  console.log('🎯↔️🎤 VAD analysis for alignment:', {
    target: {
      originalDuration: targetVadInfo.originalDuration?.toFixed(3) + 's',
      trimmedStart: targetTrimmedStart.toFixed(3) + 's',
      newDuration: targetNewDuration.toFixed(3) + 's'
    },
    user: {
      originalDuration: userVadInfo.originalDuration?.toFixed(3) + 's',
      trimmedStart: userTrimmedStart.toFixed(3) + 's',
      newDuration: userNewDuration.toFixed(3) + 's'
    }
  })
  
  // Calculate the speech onset offset - how much silence was before speech started
  const targetSpeechOnset = targetTrimmedStart
  const userSpeechOnset = userTrimmedStart
  
  // Always align user audio to match target speech onset timing
  let paddingNeeded = 0
  let alignmentType = 'none'
  
  if (targetSpeechOnset > userSpeechOnset) {
    // Target had more initial silence - pad user to match
    paddingNeeded = targetSpeechOnset - userSpeechOnset
    alignmentType = 'pad_user_start'
    console.log(`🎯↔️🎤 Target speech starts ${paddingNeeded.toFixed(3)}s later - padding user audio`)
  } else if (userSpeechOnset > targetSpeechOnset) {
    // User had more initial silence - but we still want to align speech onsets
    // So we'll pad the user to align with target's total timing structure
    const targetTotalWithOriginalSilence = targetVadInfo.originalDuration
    const targetSpeechDuration = targetNewDuration
    const targetOriginalSilenceStart = targetTotalWithOriginalSilence - targetSpeechDuration - (targetVadInfo.trimmedEnd || 0)
    
    // Align user to target's original silence structure
    paddingNeeded = Math.max(0, targetOriginalSilenceStart - userSpeechOnset)
    alignmentType = 'align_to_target_structure'
    console.log(`🎯↔️🎤 Aligning to target's original timing structure - padding ${paddingNeeded.toFixed(3)}s`)
  }
  
  if (paddingNeeded > 0.05) { // Only pad if meaningful difference (>50ms)
    console.log(`🎯↔️🎤 Applying ${alignmentType}: adding ${paddingNeeded.toFixed(3)}s padding`)
    
    const alignedUserBlob = await addSilencePadding(userBlob, paddingNeeded, true)
    
    // Update user VAD info to reflect the padding
    const newUserVadInfo = {
      ...userVadInfo,
      newDuration: userVadInfo.newDuration + paddingNeeded,
      trimmedStart: paddingNeeded > 0 ? targetSpeechOnset : userVadInfo.trimmedStart,
      aligned: true,
      paddingAdded: paddingNeeded,
      alignmentType: alignmentType
    }
    
    console.log('🎯↔️🎤 User audio aligned successfully:', {
      originalDuration: userVadInfo.newDuration.toFixed(3) + 's',
      alignedDuration: newUserVadInfo.newDuration.toFixed(3) + 's',
      paddingAdded: paddingNeeded.toFixed(3) + 's',
      alignmentType: alignmentType
    })
    
    // Update the stored VAD info
    vadTrimInfo.value = newUserVadInfo
    
    return alignedUserBlob
  } else {
    console.log('🎯↔️🎤 Speech onsets are already well aligned, no padding needed')
    return userBlob
  }
}

// Target waveform composable
const {
  wavesurfer: wavesurferTarget,
  isReady: targetReady,
  isPlaying: isTargetPlaying,
  duration: targetDurationSeconds,
  initWaveform: initTargetWaveform,
  loadAudio: loadTargetAudio,
  play: playTargetAudio,
  stop: stopTargetAudio,
  setPlaybackRate: setTargetPlaybackRate
} = useWaveform(targetWaveformRef, targetSpectrogramRef, 'target', 'target')

// User waveform composable
const {
  wavesurfer: wavesurferUser,
  isReady: userReady,
  isPlaying: isUserPlaying,
  duration: userDurationSeconds,
  initWaveform: initUserWaveform,
  loadAudio: loadUserAudio,
  play: playUserAudio,
  stop: stopUserAudio,
  setPlaybackRate: setUserPlaybackRate
} = useWaveform(userWaveformRef, userSpectrogramRef, 'user', 'user')

// Computed duration display
const targetDuration = computed(() => {
  if (targetDurationSeconds.value) {
    const mins = Math.floor(targetDurationSeconds.value / 60)
    const secs = Math.floor(targetDurationSeconds.value % 60)
    return `${mins}:${secs.toString().padStart(2, '0')}`
  }
  return '0:00'
})

// Watch for duration changes and update time sync for proportional scaling
watch(targetDurationSeconds, (newDuration) => {
  if (newDuration > 0) {
    console.log('📏 Target duration from waveform:', newDuration + 's')
    setTargetDuration(newDuration)
  }
})

watch(userDurationSeconds, (newDuration) => {
  if (newDuration > 0) {
    console.log('📏 User duration from waveform:', newDuration + 's')
    setUserDuration(newDuration)
  }
})

// Watch time sync values to debug spectrogram scaling
watch([targetWidthPercent, userWidthPercent], ([newTargetWidth, newUserWidth], [oldTargetWidth, oldUserWidth]) => {
  const alignmentInfo = vadTrimInfo.value?.aligned 
    ? `✅ Aligned (${vadTrimInfo.value.paddingAdded?.toFixed(3)}s padding added)`
    : vadTrimInfo.value 
      ? '❌ Not aligned' 
      : 'No VAD info'
  
  console.log('📐 Spectrogram scaling updated:', {
    targetWidth: { from: oldTargetWidth, to: newTargetWidth },
    userWidth: { from: oldUserWidth, to: newUserWidth },
    vadEnabled: vadEnabled.value,
    targetVadInfo: targetVadTrimInfo.value ? `${targetVadTrimInfo.value.originalDuration?.toFixed(2)}s → ${targetVadTrimInfo.value.newDuration?.toFixed(2)}s (trimmed ${targetVadTrimInfo.value.trimmedStart?.toFixed(2)}s from start)` : 'none',
    userVadInfo: vadTrimInfo.value ? `${vadTrimInfo.value.originalDuration?.toFixed(2)}s → ${vadTrimInfo.value.newDuration?.toFixed(2)}s (trimmed ${vadTrimInfo.value.trimmedStart?.toFixed(2)}s from start)` : 'none',
    alignment: alignmentInfo
  })
})

// Watch VAD enabled state and reprocess audio when toggled
watch(vadEnabled, async (newVadEnabled, oldVadEnabled) => {
  console.log('🔄 VAD toggle changed:', { from: oldVadEnabled, to: newVadEnabled })
  
  if (newVadEnabled !== oldVadEnabled) {
    console.log('🔄 Reprocessing audio with new VAD setting...')
    isVadProcessing.value = true
    
    try {
      // Reprocess target audio if available
      if (originalTargetAudio.value || trimmedTargetAudio.value) {
        console.log('🎯 Reprocessing target audio...')
        await loadTargetAudioWithVAD('/patth.wav')
      }
      
      // Reprocess user audio if available
      if (originalUserRecording.value) {
        console.log('🎤 Reprocessing user recording...')
        const processedBlob = await processUserRecordingWithVAD(originalUserRecording.value)
        const processedUrl = URL.createObjectURL(processedBlob)
        loadUserAudio(processedUrl)
      }
      
      console.log('🔄 Audio reprocessing completed')
    } finally {
      isVadProcessing.value = false
    }
  }
})

// Watch for user audio URL changes (desktop pattern)
watch(userAudioUrl, async (newUrl) => {
  if (newUrl) {
    console.log('🎵 User audio URL changed, loading into waveforms:', newUrl)
    
    // Wait for waveforms to be ready then load the audio
    await nextTick()
    if (userWaveformRef.value && userSpectrogramRef.value) {
      if (!userReady.value) {
        initUserWaveform()
        await new Promise(resolve => setTimeout(resolve, 200))
      }
      
      // Load the user audio using the existing loadUserAudio function
      try {
        await loadUserAudio(newUrl)
        console.log('✅ User audio loaded successfully via watch')
      } catch (error) {
        console.error('❌ Failed to load user audio via watch:', error)
      }
    }
  }
})

// UI state
const collapsedSections = ref({
  loading: false
})

const expandedSpectrograms = ref({
  target: false,
  user: false
})

// Computed
const recordingStatus = computed(() => {
  if (isProcessing.value) return 'Processing...'
  if (recorderIsRecording.value) return 'Recording...'
  if (userAudioUrl.value) return 'Recording complete'
  return 'Ready to record'
})

// Methods
const toggleRecordingSets = () => {
  showRecordingSetsModal.value = !showRecordingSetsModal.value
  console.log('Toggle recording sets modal:', showRecordingSetsModal.value)
}

const showOptionsMenu = () => {
  showOptionsModal.value = !showOptionsModal.value
  console.log('Show options menu:', showOptionsModal.value)
}

const getCurrentAudioName = () => {
  return currentRecording.value?.name || currentAudioName.value || 'No audio loaded'
}

const getTargetAudioUrl = () => {
  return currentRecording.value?.audioUrl || targetAudioUrl.value
}

const showLoadOptions = () => {
  showOptionsModal.value = true
}

const triggerFileInput = () => {
  hiddenFileInput.value?.click()
}

// Utility function for getting audio duration (same as desktop)
const getAudioDuration = async (audioBlob) => {
  return new Promise((resolve, reject) => {
    const audio = new Audio()
    audio.onloadedmetadata = () => {
      resolve(audio.duration)
    }
    audio.onerror = () => {
      reject(new Error('Failed to load audio for duration calculation'))
    }
    audio.src = URL.createObjectURL(audioBlob)
  })
}

const setTargetAudio = async (audioBlob, source = {}) => {
  if (!audioBlob) {
    console.warn('🎯 setTargetAudio: No audio blob provided')
    // Clear everything
    const oldTargetUrl = targetAudioUrl.value
    targetAudioUrl.value = null
    currentAudioName.value = 'No audio loaded'
    
    // Cleanup old blob URL
    if (oldTargetUrl && oldTargetUrl.startsWith('blob:')) {
      setTimeout(() => {
        URL.revokeObjectURL(oldTargetUrl)
      }, 3000)
    }
    return
  }

  try {
    const sourceType = source.source || 'manual'
    
    // Store old URL for cleanup
    const oldTargetUrl = targetAudioUrl.value
    
    // Get raw duration for debugging
    const rawDuration = await getAudioDuration(audioBlob)
    
    // Process target audio with VAD
    const targetProcessed = await processAudio(audioBlob, {
      positiveSpeechThreshold: 0.3,
      negativeSpeechThreshold: 0.2,
      minSpeechFrames: 3,
      redemptionFrames: 32,
      padding: 0.05
    })
    
    if (targetProcessed.processed && targetProcessed.vadBoundaries) {
      // Normalize target audio to have consistent padding
      const normalizedBlob = await normalizeAudioSilence(
        targetProcessed.audioBlob,
        targetProcessed.vadBoundaries,
        200 // 200ms padding
      )
      
      // Update state with processed audio
      targetAudioUrl.value = URL.createObjectURL(normalizedBlob)
      currentAudioName.value = source.name || source.fileName || 'Loaded audio'
      
      console.log('✅ Target audio processed successfully')
    } else {
      console.log('📏 Target VAD processing failed - using original audio')
      
      // Use original audio when VAD processing fails
      targetAudioUrl.value = URL.createObjectURL(audioBlob)
      currentAudioName.value = source.name || source.fileName || 'Loaded audio'
    }
    
    // Cleanup old blob URL
    if (oldTargetUrl && oldTargetUrl.startsWith('blob:')) {
      setTimeout(() => {
        URL.revokeObjectURL(oldTargetUrl)
      }, 3000)
    }
    
    // Initialize target waveform with new audio
    setTimeout(() => {
      if (targetAudioUrl.value) {
        loadTargetAudio(targetAudioUrl.value)
      }
    }, 100)
    
  } catch (error) {
    console.error('Error setting target audio:', error)
  }
}

const handleFileSelection = async (event) => {
  const file = event.target.files[0]
  if (file) {
    await setTargetAudio(file, { name: file.name, fileName: file.name })
    
    // Reset the input so the same file can be selected again if needed
    event.target.value = ''
  }
}

const handleUrlLoad = async () => {
  const url = tempAudioUrl.value.trim()
  if (!url) return
  
  try {
    // Validate URL format
    new URL(url)
    
    // Fetch the audio file
    const response = await fetch(url)
    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`)
    }
    
    // Convert to blob
    const blob = await response.blob()
    
    // Validate it's an audio file
    if (!blob.type.startsWith('audio/')) {
      throw new Error('URL does not point to an audio file')
    }
    
    // Use unified processing
    await setTargetAudio(blob, { url: url, name: url })
    
    showUrlModal.value = false
    tempAudioUrl.value = ''
    
  } catch (error) {
    console.error('Error loading audio from URL:', error)
    alert(`Failed to load audio from URL: ${error.message}`)
  }
}

// Recording sets are managed by the desktop version
// Mobile version displays the currently active set

const selectFile = () => {
  console.log('Select file')
  showOptionsModal.value = false
  triggerFileInput()
}

const showUrlInput = () => {
  console.log('Show URL input')
  showOptionsModal.value = false
  showUrlModal.value = true
}

const showFolderUpload = () => {
  console.log('Show folder upload')
  showOptionsModal.value = false
  // TODO: Implement folder upload modal
}

const showSettings = () => {
  console.log('Show settings')
  showOptionsModal.value = false
  // TODO: Implement settings modal
}

const toggleSection = (section) => {
  collapsedSections.value[section] = !collapsedSections.value[section]
}

// Function to load target audio with VAD processing
const loadTargetAudioWithVAD = async (audioUrl) => {
  console.log('🎯 Loading target audio with VAD processing:', audioUrl)
  
  isVadProcessing.value = true
  
  try {
    // Fetch the audio file
    const response = await fetch(audioUrl)
    const audioBlob = await response.blob()
    
    console.log('🎯 Target audio fetched, applying VAD processing...')
    
    // Process with VAD trimming
    const processedBlob = await processTargetAudioWithVAD(audioBlob)
    
    // Create blob URL for the processed audio
    const processedUrl = URL.createObjectURL(processedBlob)
    
    // Load the processed audio into the target waveform
    loadTargetAudio(processedUrl)
    
    console.log('🎯 Target audio loaded with VAD processing')
    
    // Show VAD trim info if available
    if (targetVadTrimInfo.value) {
      console.log('🎯 Target VAD Trim Results:', targetVadTrimInfo.value)
    }
    
  } catch (error) {
    console.error('🎯 Error loading target audio with VAD:', error)
    console.log('🎯 Falling back to direct audio loading')
    // Fallback to direct loading
    loadTargetAudio(audioUrl)
  } finally {
    isVadProcessing.value = false
  }
}

const initializeTargetWaveform = async () => {
  await nextTick()

  if (targetWaveformRef.value && targetSpectrogramRef.value) {
    console.log('Initializing target waveform...')
    initTargetWaveform()

    // Wait for initialization then load and process audio with VAD
    setTimeout(async () => {
      await loadTargetAudioWithVAD('/patth.wav')
    }, 100)
  } else {
    console.warn('Target waveform containers not ready, retrying...')
    // Retry after a delay for better HMR stability
    setTimeout(async () => {
      await nextTick()
      if (targetWaveformRef.value && targetSpectrogramRef.value) {
        console.log('Retry successful - initializing target waveform...')
        initTargetWaveform()
        setTimeout(async () => {
          await loadTargetAudioWithVAD('/patth.wav')
        }, 100)
      } else {
        console.error('Refs still not available after retry')
      }
    }, 100)
  }
}

// Simplified user waveform initialization (follows desktop pattern)
const initializeUserWaveform = async () => {
  await nextTick()

  if (userWaveformRef.value && userSpectrogramRef.value) {
    console.log('🎵 Initializing user waveform containers...')
    initUserWaveform()
    console.log('✅ User waveform containers ready')
  } else {
    console.warn('⚠️ User waveform containers not ready, retrying...')
    setTimeout(async () => {
      await nextTick()
      if (userWaveformRef.value && userSpectrogramRef.value) {
        initUserWaveform()
        console.log('✅ User waveform containers ready after retry')
      }
    }, 100)
  }
}

const playTarget = () => {
  if (targetReady.value) {
    if (isTargetPlaying.value) {
      stopTargetAudio()
    } else {
      playTargetAudio()
    }
  }
}

const playUser = () => {
  if (userReady.value) {
    if (isUserPlaying.value) {
      stopUserAudio()
    } else {
      playUserAudio()
    }
  } else {
    console.log('User audio not ready yet')
  }
}

// Function to process target audio with VAD trimming
const processTargetAudioWithVAD = async (audioBlob) => {
  console.log('🎯 Processing target audio with VAD trimming...')
  
  if (!vadEnabled.value) {
    console.log('🎯 VAD disabled, using original target audio')
    trimmedTargetAudio.value = audioBlob
    return audioBlob
  }
  
  try {
    // Store original target audio
    originalTargetAudio.value = audioBlob
    
    // Apply VAD trimming with exact same settings as desktop
    const vadOptions = {
      padding: 0.2,                    // 200ms padding around speech (desktop default)
      threshold: 0.25,                 // VAD sensitivity (desktop default)
      minSpeechDuration: 50,           // 50ms minimum speech segment
      maxSilenceDuration: 500,         // 500ms maximum gap between speech segments  
      maxTrimStart: 3.0,               // Max 3 seconds of silence to trim from start
      maxTrimEnd: 2.0,                 // Max 2 seconds of silence to trim from end
    }
    
    console.log('🎯 Applying VAD trimming to target audio with options:', vadOptions)
    const trimResult = await trimAudioWithVAD(audioBlob, vadOptions)
    
    if (trimResult && trimResult.blob) {
      console.log('🎯 Target audio VAD trimming successful:', {
        originalDuration: trimResult.originalDuration?.toFixed(2) + 's',
        newDuration: trimResult.newDuration?.toFixed(2) + 's',
        trimmedStart: trimResult.trimmedStart?.toFixed(2) + 's',
        trimmedEnd: trimResult.trimmedEnd?.toFixed(2) + 's'
      })
      
      // Store trimmed target audio and info
      trimmedTargetAudio.value = trimResult.blob
      targetVadTrimInfo.value = {
        originalDuration: trimResult.originalDuration,
        newDuration: trimResult.newDuration,
        trimmedStart: trimResult.trimmedStart,
        trimmedEnd: trimResult.trimmedEnd,
        boundaries: trimResult.boundaries
      }
      
      // Explicitly update time sync with trimmed duration for accurate scaling
      if (trimResult.newDuration > 0) {
        console.log('🎯 Updating time sync with VAD-trimmed target duration:', trimResult.newDuration)
        setTargetDuration(trimResult.newDuration)
      }
      
      return trimResult.blob
    } else {
      console.warn('🎯 Target audio VAD trimming failed, using original audio')
      trimmedTargetAudio.value = audioBlob
      return audioBlob
    }
  } catch (error) {
    console.error('🎯 Error during target audio VAD processing:', error)
    console.log('🎯 Falling back to original target audio')
    trimmedTargetAudio.value = audioBlob
    return audioBlob
  }
}

// Function to process user recording with VAD trimming
const processUserRecordingWithVAD = async (audioBlob) => {
  console.log('🎤 Processing user recording with VAD trimming...')
  
  if (!vadEnabled.value) {
    console.log('🎤 VAD disabled, using original recording')
    trimmedUserRecording.value = audioBlob
    return audioBlob
  }
  
  try {
    // Store original recording
    originalUserRecording.value = audioBlob
    
    // Apply VAD trimming with exact same settings as desktop
    const vadOptions = {
      padding: 0.2,                    // 200ms padding around speech (desktop default)
      threshold: 0.25,                 // VAD sensitivity (desktop default)
      minSpeechDuration: 50,           // 50ms minimum speech segment
      maxSilenceDuration: 500,         // 500ms maximum gap between speech segments  
      maxTrimStart: 3.0,               // Max 3 seconds of silence to trim from start
      maxTrimEnd: 2.0,                 // Max 2 seconds of silence to trim from end
    }
    
    console.log('🎤 Applying VAD trimming with options:', vadOptions)
    const trimResult = await trimAudioWithVAD(audioBlob, vadOptions)
    
    if (trimResult && trimResult.blob) {
      console.log('🎤 VAD trimming successful:', {
        originalDuration: trimResult.originalDuration?.toFixed(2) + 's',
        newDuration: trimResult.newDuration?.toFixed(2) + 's',
        trimmedStart: trimResult.trimmedStart?.toFixed(2) + 's',
        trimmedEnd: trimResult.trimmedEnd?.toFixed(2) + 's'
      })
      
      // Store trimmed recording and info
      trimmedUserRecording.value = trimResult.blob
      vadTrimInfo.value = {
        originalDuration: trimResult.originalDuration,
        newDuration: trimResult.newDuration,
        trimmedStart: trimResult.trimmedStart,
        trimmedEnd: trimResult.trimmedEnd,
        boundaries: trimResult.boundaries
      }
      
      // Apply intelligent alignment with target audio
      let finalBlob = trimResult.blob
      if (targetVadTrimInfo.value) {
        console.log('🎤 Applying intelligent alignment with target audio...')
        finalBlob = await alignAudioWithVADInfo(trimResult.blob, vadTrimInfo.value, targetVadTrimInfo.value)
      } else {
        console.log('🎤 No target VAD info available, skipping alignment')
      }
      
      // Update time sync with final duration (may include alignment padding)
      const finalDuration = vadTrimInfo.value.newDuration
      if (finalDuration > 0) {
        console.log('🎤 Updating time sync with final aligned user duration:', finalDuration)
        setUserDuration(finalDuration)
      }
      
      return finalBlob
    } else {
      console.warn('🎤 VAD trimming failed, using original recording')
      trimmedUserRecording.value = audioBlob
      return audioBlob
    }
  } catch (error) {
    console.error('🎤 Error during VAD processing:', error)
    console.log('🎤 Falling back to original recording')
    trimmedUserRecording.value = audioBlob
    return audioBlob
  }
}

const toggleRecording = async () => {
  if (recorderIsRecording.value) {
    // Stop recording
    await stopRecording()
  } else {
    // Start recording
    await startRecording()
  }
}

const cycleSpeed = () => {
  // Cycle to next speed option
  currentSpeedIndex.value = (currentSpeedIndex.value + 1) % speedOptions.length
  const newSpeed = currentSpeed.value
  
  console.log('Cycle speed to:', newSpeed)
  
  // Apply speed to both target and user audio
  if (targetReady.value) {
    setTargetPlaybackRate(newSpeed)
    console.log('Applied speed', newSpeed, 'to target audio')
  }
  
  if (userReady.value) {
    setUserPlaybackRate(newSpeed)
    console.log('Applied speed', newSpeed, 'to user audio')
  }
}

const adjustSpeed = (speedValue) => {
  let rate = 1.0
  if (speedValue === -0.25) rate = 0.5
  else if (speedValue === 0) rate = 1.0
  else if (speedValue === 0.25) rate = 1.5

  console.log('Adjust speed to:', rate)
  if (targetReady.value) {
    setTargetPlaybackRate(rate)
  }
}

const toggleSpectrogram = (type) => {
  expandedSpectrograms.value[type] = !expandedSpectrograms.value[type]
}

const retryRecording = () => {
  hasUserRecording.value = false
  console.log('Retry recording')
}

const enhanceRecording = () => {
  console.log('Enhance recording')
}

const playOverlapping = () => {
  console.log('Play overlapping view')
  // Toggle overlay mode for waveforms
  overlayMode.value = !overlayMode.value
}

const startRecording = async () => {
  try {
    console.log('🎤 Starting recording...')
    if (recorderIsRecording.value) {
      // If already recording, stop it
      await stopRecording()
    } else {
      // Start recording
      await startAudioRecording()
    }
  } catch (error) {
    console.error('❌ Failed to start recording:', error)
    alert('Could not start recording. Please ensure you have a microphone and have granted permission.')
  }
}

const stopRecording = async () => {
  try {
    console.log('⏹️ Stopping recording...')
    isProcessing.value = true
    
    // Get the recorded audio blob
    const recordedBlob = await stopAudioRecording()
    
    if (recordedBlob) {
      console.log('✅ Recording captured, processing...')
      
      // Use desktop pattern for handling recorded audio
      await handleRecordedAudio(recordedBlob)
    } else {
      console.warn('⚠️ No recording data received')
      isProcessing.value = false
    }
  } catch (error) {
    console.error('❌ Failed to stop recording:', error)
    isProcessing.value = false
  }
}

// Desktop pattern: simple and clean recording handling
const handleRecordedAudio = async (blob) => {
  try {
    // Validate the audio blob (same as desktop)
    if (!blob || blob.size === 0) {
      console.warn('🎤 Empty or invalid audio blob received, skipping processing')
      isProcessing.value = false
      return
    }
    
    // Create URL and set basic state (same as desktop)
    userAudioBlob.value = blob
    userAudioUrl.value = URL.createObjectURL(blob)
    hasUserRecording.value = true
    
    console.log('✅ User audio URL created:', userAudioUrl.value)
    
    // Update recording set if active (use existing desktop function)
    if (currentRecording.value && typeof updateUserRecording === 'function') {
      updateUserRecording(blob, userAudioUrl.value)
      console.log('💾 Recording updated in recording set')
    }
    
    // Auto-play both audios (same as desktop)
    if (autoPlayEnabled.value) {
      console.log('🎵 Auto-playing both audios...')
      setTimeout(() => {
        if (targetReady.value && userReady.value) {
          playBoth()
        } else {
          console.log('⚠️ Waveforms not ready for auto-play, skipping')
        }
      }, 1000)
    }
    
    isProcessing.value = false
    
  } catch (error) {
    console.error('❌ Failed to handle recorded audio:', error)
    isProcessing.value = false
  }
}


const saveRecording = () => {
  console.log('Save recording')
  if (currentRecording.value && hasUserRecording.value) {
    // TODO: Save user recording to current recording set
    console.log('Recording saved for:', currentRecording.value.name)
  }
}

const playBoth = async () => {
  console.log('Play both audios sequentially (target first, then user)')
  
  // Stop any currently playing audio first
  if (isTargetPlaying.value) {
    stopTargetAudio()
  }
  if (isUserPlaying.value) {
    stopUserAudio()
  }
  
  if (targetReady.value && userReady.value) {
    console.log('Both audios ready - starting sequential playback')
    
    try {
      // Play target audio first
      console.log('Playing target audio...')
      playTargetAudio()
      
      // Wait for target audio to finish
      await new Promise((resolve) => {
        const checkFinished = () => {
          if (!isTargetPlaying.value) {
            console.log('Target audio finished, now playing user audio...')
            resolve()
          } else {
            setTimeout(checkFinished, 100)
          }
        }
        
        // Start checking after a brief delay
        setTimeout(checkFinished, 200)
      })
      
      // Now play user audio
      playUserAudio()
      console.log('Sequential playback initiated successfully')
      
    } catch (error) {
      console.error('Error during sequential playback:', error)
    }
  } else if (targetReady.value) {
    console.log('Only target audio is ready, playing target only')
    playTargetAudio()
  } else if (userReady.value) {
    console.log('Only user audio is ready, playing user only')
    playUserAudio()
  } else {
    console.log('No audio is ready to play')
  }
}

const toggleOverlay = () => {
  overlayMode.value = !overlayMode.value
}

const syncPlayback = () => {
  console.log('Sync playback')
}

// These functions are already imported from useRecordingSets composable

// Reset user recording and reload demo audio
const resetUserRecording = async () => {
  console.log('🔄 Reset user recording triggered')
  
  isProcessing.value = true
  hasUserRecording.value = false
  
  try {
    // Clear existing user audio
    vadTrimInfo.value = null
    originalUserRecording.value = null
    trimmedUserRecording.value = null
    
    // Reset user duration in time sync
    setUserDuration(0)
    
    // Small delay for UI update
    await new Promise(resolve => setTimeout(resolve, 500))
    
    // Reload the demo user audio file
    const response = await fetch('/path.mp3')
    const audioBlob = await response.blob()
    
    console.log('🔄 Processing demo audio with VAD...')
    
    // Store original for reprocessing
    originalUserRecording.value = audioBlob
    
    // Process with VAD trimming and alignment
    const processedBlob = await processUserRecordingWithVAD(audioBlob)
    
    // Create blob URL for the processed audio
    const processedUrl = URL.createObjectURL(processedBlob)
    
    // Load the processed audio into the user waveform
    loadUserAudio(processedUrl)
    
    isProcessing.value = false
    hasUserRecording.value = true
    
    console.log('🔄 User recording reset completed')
    
  } catch (error) {
    console.error('🔄 Error resetting user recording:', error)
    isProcessing.value = false
    hasUserRecording.value = false
  }
}

// Manual alignment function
const manuallyAlignAudio = async () => {
  console.log('🎯 Manual alignment triggered')
  
  if (!originalUserRecording.value) {
    console.warn('🎯 Cannot align: no user recording available')
    return
  }
  
  if (!targetVadTrimInfo.value) {
    console.warn('🎯 Cannot align: target audio VAD info not available')
    return
  }
  
  isVadProcessing.value = true
  
  try {
    console.log('🎯 Forcing re-alignment of user audio...')
    
    // Clear existing alignment info to force recalculation
    if (vadTrimInfo.value) {
      vadTrimInfo.value.aligned = false
      vadTrimInfo.value.paddingAdded = 0
    }
    
    // Reprocess user recording with VAD and alignment
    const reprocessedBlob = await processUserRecordingWithVAD(originalUserRecording.value)
    
    // Create new blob URL
    const processedUrl = URL.createObjectURL(reprocessedBlob)
    
    // Load the reprocessed and aligned audio
    loadUserAudio(processedUrl)
    
    console.log('🎯 Manual alignment completed')
    
    // Show alignment info if available
    if (vadTrimInfo.value?.aligned) {
      console.log('🎯 Alignment applied:', {
        paddingAdded: vadTrimInfo.value.paddingAdded?.toFixed(3) + 's',
        alignmentType: vadTrimInfo.value.alignmentType,
        finalDuration: vadTrimInfo.value.newDuration?.toFixed(3) + 's'
      })
    }
  } catch (error) {
    console.error('🎯 Error during manual alignment:', error)
  } finally {
    isVadProcessing.value = false
  }
}

onMounted(async () => {
  console.log('Mobile Test A mounted')

  // Same initialization pattern as desktop
  await initDB()
  
  // Initialize VAD model for professional audio processing
  try {
    await initVAD()
    if (vadReady.value) {
      console.log('✅ VAD processor initialized successfully')
    } else {
      console.log('⚠️ VAD processor not ready after initialization')
    }
  } catch (error) {
    console.warn('⚠️ VAD initialization failed:', error.message)
    console.error('🔍 VAD Error details:', error)
  }

  // Initialize waveforms after component is mounted
  await initializeTargetWaveform()
  await initializeUserWaveform()
  
  // Auto-load patth.wav from public directory
  try {
    const response = await fetch('/patth.wav')
    if (response.ok) {
      const blob = await response.blob()
      await setTargetAudio(blob, { name: 'patth.wav', fileName: 'patth.wav' })
      console.log('✅ Auto-loaded patth.wav')
    }
  } catch (error) {
    console.log('ℹ️ patth.wav not found in public directory, will load on user action')
  }
})
</script>

<style scoped>
.mobile-practice-a {
  min-height: 100vh;
  background: #f8fafc;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
}

/* Mobile Header */
.mobile-header {
  position: sticky;
  top: 0;
  z-index: 100;
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: 12px 16px;
  background: #ffffff;
  border-bottom: 1px solid #e5e7eb;
  box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
}

.back-btn,
.settings-btn {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 44px;
  height: 44px;
  border: none;
  border-radius: 12px;
  background: #f3f4f6;
  cursor: pointer;
  transition: all 0.2s;
}

.back-btn:hover,
.settings-btn:hover {
  background: #e5e7eb;
}

.header-title {
  flex: 1;
  text-align: center;
  margin: 0 12px;
}

.header-title h1 {
  font-size: 18px;
  font-weight: 600;
  color: #1f2937;
  margin: 0;
  line-height: 1.2;
}

.header-title .progress {
  font-size: 14px;
  color: #6b7280;
  margin-top: 2px;
}

/* Audio Loading Control */
.audio-loading-control {
  background: white;
  border: 1px solid #d1d5db;
  border-radius: 8px;
  margin: 8px 16px;
  overflow: hidden;
}

.loading-header {
  background: #f8fafc;
  padding: 8px 12px;
  font-size: 12px;
  font-weight: 600;
  color: #374151;
  text-align: center;
  border-bottom: 1px solid #e5e7eb;
}

.loading-content {
  padding: 12px;
  display: flex;
  align-items: center;
  justify-content: space-between;
}

.current-file {
  font-size: 14px;
  font-weight: 500;
  color: #374151;
  flex: 1;
  min-width: 0;
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

.loading-actions {
  display: flex;
  gap: 8px;
  margin-left: 12px;
}

.load-btn, .speed-btn {
  padding: 6px 12px;
  border: 1px solid #3b82f6;
  border-radius: 6px;
  background: white;
  color: #3b82f6;
  font-size: 12px;
  font-weight: 500;
  cursor: pointer;
  transition: all 0.2s;
}

.load-btn:hover, .speed-btn:hover {
  background: #eff6ff;
}

/* Compact Audio Info */
.audio-info-compact {
  background: white;
  margin: 4px 16px;
  padding: 8px 16px;
  border: 1px solid #d1d5db;
  border-radius: 6px;
  display: flex;
  justify-content: space-between;
  align-items: center;
  font-size: 14px;
}

.audio-controls {
  display: flex;
  gap: 8px;
}

.audio-controls .load-btn {
  padding: 6px 12px;
  font-size: 12px;
  border-radius: 4px;
  border: 1px solid #d1d5db;
  background: #f9fafb;
  color: #374151;
  cursor: pointer;
  transition: all 0.2s;
  display: flex;
  align-items: center;
  gap: 4px;
}

.audio-controls .load-btn:hover {
  background: #f3f4f6;
  border-color: #9ca3af;
}

.audio-controls .btn-icon {
  font-size: 14px;
}

.audio-controls .btn-text {
  font-size: 11px;
  font-weight: 500;
}

/* Audio Sections - Reduced Spacing */
.audio-section {
  background: white;
  margin: 4px 16px;
  border: 1px solid #d1d5db;
  border-radius: 8px;
  overflow: hidden;
}

.section-label {
  background: #f8fafc;
  padding: 4px 12px;
  font-size: 11px;
  font-weight: 700;
  color: #374151;
  text-align: center;
  border-bottom: 1px solid #e5e7eb;
  letter-spacing: 0.5px;
}

/* Compact Controls Section */
.controls-section {
  background: white;
  margin: 8px 16px;
  padding: 12px;
  border: 1px solid #d1d5db;
  border-radius: 8px;
}

.playback-row {
  display: flex;
  gap: 8px;
  margin-bottom: 8px;
}

.record-row {
  display: flex;
  justify-content: center;
}

/* Button styles removed - now using PlaybackButton component */

.waveform-container {
  height: 50px;
  width: 100%;
  background: #fafbfc;
  position: relative;
  border-bottom: 1px solid #e5e7eb;
  z-index: 2;
}

.spectrogram-container {
  height: 70px;
  width: 100%;
  background: #fafbfc;
  position: relative;
  z-index: 1;
}

.spectrogram-container.large {
  height: 100px;
}

.placeholder {
  display: flex;
  align-items: center;
  justify-content: center;
  height: 100%;
  font-size: 12px;
  color: #9ca3af;
  text-align: center;
  padding: 8px;
}

.placeholder.large {
  font-size: 16px;
  font-weight: 700;
  color: #6b7280;
  letter-spacing: 1px;
}

/* Recording Placeholder */
.recording-placeholder {
  display: flex;
  align-items: center;
  justify-content: center;
  height: 120px;
  background: #f8fafc;
}

.record-button-large {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  width: 80px;
  height: 80px;
  border: 2px solid #10b981;
  border-radius: 50%;
  background: linear-gradient(135deg, #10b981 0%, #047857 100%);
  color: white;
  cursor: pointer;
  transition: all 0.3s;
  box-shadow: 0 4px 12px rgba(16, 185, 129, 0.3);
}

.record-button-large:hover {
  transform: scale(1.05);
  box-shadow: 0 6px 16px rgba(16, 185, 129, 0.4);
}

.record-button-large:disabled {
  opacity: 0.5;
  cursor: not-allowed;
  transform: none;
}

.record-icon {
  font-size: 24px;
  margin-bottom: 4px;
}

.record-text {
  font-size: 10px;
  font-weight: 600;
  text-align: center;
}

/* Alignment Status */
.alignment-status {
  position: absolute;
  bottom: 4px;
  right: 8px;
  display: flex;
  align-items: center;
  gap: 4px;
  padding: 2px 6px;
  background: rgba(34, 197, 94, 0.9);
  color: white;
  border-radius: 4px;
  font-size: 10px;
  font-weight: 500;
}

.status-icon {
  font-size: 10px;
}

.status-text {
  font-size: 9px;
}

.section-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: 16px;
  background: #f8fafc;
  border-bottom: 1px solid #e5e7eb;
}

.section-header h2 {
  font-size: 16px;
  font-weight: 600;
  color: #374151;
  margin: 0;
}

.section-content {
  padding: 16px;
}

/* Compact Audio Loading */
.compact-loading-section {
  background: white;
  border-bottom: 1px solid #e5e7eb;
  padding: 0;
  margin: 0;
}

.loading-bar {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: 8px 16px;
  background: #f8fafc;
  border-bottom: 1px solid #e5e7eb;
}

.current-file {
  font-size: 13px;
  color: #374151;
  font-weight: 500;
  flex: 1;
  min-width: 0;
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

.loading-actions {
  display: flex;
  gap: 6px;
  margin-left: 12px;
}

.action-btn {
  width: 32px;
  height: 32px;
  border: 1px solid #d1d5db;
  border-radius: 6px;
  background: white;
  color: #374151;
  font-size: 14px;
  cursor: pointer;
  transition: all 0.2s;
  display: flex;
  align-items: center;
  justify-content: center;
}

.action-btn:hover {
  background: #f3f4f6;
  border-color: #9ca3af;
}

.file-btn:hover {
  color: #3b82f6;
}

.url-btn:hover {
  color: #10b981;
}

/* Audio Section */
.audio-section {
  margin-bottom: 0;
}

.target-section .section-header {
  background: linear-gradient(135deg, #3b82f6 0%, #1d4ed8 100%);
  color: white;
}

.user-section .section-header {
  background: linear-gradient(135deg, #10b981 0%, #047857 100%);
  color: white;
}

/* Recording Controls */
.recording-controls {
  background: white;
  border: 1px solid #d1d5db;
  border-radius: 8px;
  margin: 8px 16px;
  padding: 16px;
}

.controls-container {
  display: flex;
  align-items: center;
  justify-content: space-between;
  gap: 12px;
  flex-wrap: wrap;
}

/* Play button styles removed - now using PlaybackButton component */

.play-icon {
  font-size: 18px;
  color: #3b82f6;
  margin-bottom: 4px;
}

.play-label {
  font-size: 10px;
  font-weight: 600;
  color: #374151;
  text-align: center;
}

.speed-controls {
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: 4px;
}

.speed-label {
  font-size: 9px;
  font-weight: 500;
  color: #6b7280;
  text-align: center;
}

.speed-control .speed-btn {
  padding: 6px 12px;
  border: 1px solid #f59e0b;
  border-radius: 6px;
  background: white;
  color: #f59e0b;
  font-size: 12px;
  font-weight: 600;
  cursor: pointer;
  transition: all 0.2s;
}

.speed-control .speed-btn:hover {
  background: #fffbeb;
}

.record-btn {
  display: flex;
  flex-direction: column;
  align-items: center;
  padding: 8px 12px;
  border: 2px solid #ef4444;
  border-radius: 8px;
  background: white;
  cursor: pointer;
  transition: all 0.2s;
  min-width: 80px;
}

.record-btn:hover:not(:disabled) {
  background: #fef2f2;
  transform: translateY(-1px);
}

.record-btn:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.record-icon {
  font-size: 18px;
  color: #ef4444;
  margin-bottom: 4px;
}

.record-label {
  font-size: 10px;
  font-weight: 600;
  color: #374151;
  text-align: center;
}


.control-btn {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  padding: 8px 12px;
  border: 1px solid #d1d5db;
  border-radius: 8px;
  background: white;
  cursor: pointer;
  transition: all 0.2s;
  min-width: 64px;
  font-size: 12px;
  flex: 1;
  max-width: 80px;
}

.control-btn-sm {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  padding: 6px 8px;
  border: 1px solid #d1d5db;
  border-radius: 6px;
  background: white;
  cursor: pointer;
  transition: all 0.2s;
  min-width: 48px;
  font-size: 10px;
}

.play-both {
  border-color: #8b5cf6;
  color: #8b5cf6;
}

.play-both:hover:not(:disabled) {
  background: #f3f4f6;
  border-color: #7c3aed;
}

.record-btn {
  border-color: #ef4444;
  color: #ef4444;
}

.record-btn:hover:not(:disabled) {
  background: #fef2f2;
  border-color: #dc2626;
}

.align-btn {
  border-color: #f59e0b;
  color: #f59e0b;
}

.align-btn:hover:not(:disabled) {
  background: #fffbeb;
  border-color: #d97706;
}

.vad-toggle {
  border-color: #6b7280;
  color: #6b7280;
}

.vad-toggle.active {
  border-color: #10b981;
  color: #10b981;
  background: #ecfdf5;
}

.save-btn {
  border-color: #22c55e;
  color: #22c55e;
}

.save-btn:hover:not(:disabled) {
  background: #f0fdf4;
  border-color: #16a34a;
}

.control-btn:hover:not(:disabled) {
  background: #f3f4f6;
  border-color: #9ca3af;
  transform: translateY(-1px);
}

.control-btn:disabled {
  opacity: 0.5;
  cursor: not-allowed;
  transform: none;
}

.play-target {
  border-color: #3b82f6;
  color: #3b82f6;
}

.play-target:hover:not(:disabled) {
  background: #eff6ff;
  border-color: #2563eb;
}

.play-user {
  border-color: #10b981;
  color: #10b981;
}

.play-user:hover:not(:disabled) {
  background: #ecfdf5;
  border-color: #059669;
}

.play-both {
  border-color: #8b5cf6;
  color: #8b5cf6;
}

.play-both:hover:not(:disabled) {
  background: #f3f4f6;
  border-color: #7c3aed;
}

.record-again {
  border-color: #ef4444;
  color: #ef4444;
}

.record-again:hover:not(:disabled) {
  background: #fef2f2;
  border-color: #dc2626;
}

.btn-icon {
  font-size: 16px;
  margin-bottom: 2px;
}

.btn-text {
  font-size: 10px;
  font-weight: 500;
  text-align: center;
  line-height: 1.1;
}

.sm-icon {
  font-size: 14px;
  margin-bottom: 2px;
}

.sm-text {
  font-size: 9px;
  font-weight: 500;
  text-align: center;
  line-height: 1.1;
}

/* Bottom Navigation */
.bottom-navigation {
  position: sticky;
  bottom: 0;
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: 12px 16px;
  background: white;
  border-top: 1px solid #e5e7eb;
  box-shadow: 0 -2px 8px rgba(0, 0, 0, 0.1);
}

.nav-btn {
  display: flex;
  flex-direction: column;
  align-items: center;
  padding: 8px 12px;
  border: 1px solid #d1d5db;
  border-radius: 8px;
  background: white;
  cursor: pointer;
  transition: all 0.2s;
  min-width: 60px;
}

.nav-btn:hover:not(:disabled) {
  background: #f3f4f6;
  border-color: #9ca3af;
}

.nav-btn:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.nav-icon {
  font-size: 16px;
  margin: 2px 0;
}

.nav-text {
  font-size: 10px;
  font-weight: 500;
  color: #374151;
}

.nav-center {
  display: flex;
  flex-direction: column;
  align-items: center;
  text-align: center;
}

.nav-progress {
  font-size: 12px;
  color: #6b7280;
  margin-bottom: 4px;
}

.complete-btn {
  padding: 6px 12px;
  border: 1px solid #22c55e;
  border-radius: 6px;
  background: #22c55e;
  color: white;
  font-size: 11px;
  font-weight: 600;
  cursor: pointer;
  transition: all 0.2s;
}

.complete-btn:hover {
  background: #16a34a;
  border-color: #16a34a;
}

/* Bottom Safe Area */
.bottom-safe-area {
  height: env(safe-area-inset-bottom, 0px);
  background: white;
}

/* Header Button Styling */
.header-btn {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 40px;
  height: 40px;
  border: none;
  border-radius: 10px;
  background: #f3f4f6;
  cursor: pointer;
  transition: all 0.2s;
}

.header-btn:hover {
  background: #e5e7eb;
}

.header-btn .icon {
  font-size: 16px;
  color: #374151;
}

/* Responsive Design */
@media (max-width: 480px) {
  .controls-container {
    justify-content: center;
    gap: 8px;
  }
  
  .play-btn, .record-btn {
    min-width: 50px;
    padding: 6px 8px;
  }
  
  .play-icon, .record-icon {
    font-size: 16px;
  }
  
  .play-label, .record-label {
    font-size: 9px;
  }
  
  .speed-controls {
    order: -1;
    width: 100%;
    margin-bottom: 8px;
  }
}

@media (max-width: 360px) {
  .compact-visualizations {
    height: 90px;
  }
  
  .control-btn {
    min-width: 50px;
    padding: 5px 6px;
  }
  
  .btn-text {
    font-size: 8px;
  }
}

/* Modal Styles */
.modal-overlay {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: rgba(0, 0, 0, 0.5);
  backdrop-filter: blur(4px);
  z-index: 1000;
  display: flex;
  align-items: center;
  justify-content: center;
  padding: 20px;
}

.modal-content {
  background: white;
  border-radius: 16px;
  box-shadow: 0 20px 25px -5px rgba(0, 0, 0, 0.1);
  max-width: 400px;
  width: 100%;
  max-height: 80vh;
  overflow: hidden;
  animation: modalSlideIn 0.3s ease-out;
}

@keyframes modalSlideIn {
  from {
    opacity: 0;
    transform: translateY(20px) scale(0.95);
  }
  to {
    opacity: 1;
    transform: translateY(0) scale(1);
  }
}

.modal-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: 20px 24px 16px;
  border-bottom: 1px solid #e5e7eb;
}

.modal-header h3 {
  font-size: 18px;
  font-weight: 600;
  color: #1f2937;
  margin: 0;
}

.close-btn {
  width: 32px;
  height: 32px;
  border: none;
  border-radius: 8px;
  background: #f3f4f6;
  color: #6b7280;
  font-size: 20px;
  font-weight: 300;
  cursor: pointer;
  transition: all 0.2s;
  display: flex;
  align-items: center;
  justify-content: center;
}

.close-btn:hover {
  background: #e5e7eb;
  color: #374151;
}

.modal-body {
  padding: 16px 24px 24px;
  max-height: calc(80vh - 100px);
  overflow-y: auto;
}

/* Recording Sets Modal */
.empty-state {
  text-align: center;
  padding: 40px 20px;
}

.empty-state p {
  color: #6b7280;
  margin-bottom: 16px;
}

.add-btn {
  padding: 8px 16px;
  border: 1px solid #3b82f6;
  border-radius: 8px;
  background: #3b82f6;
  color: white;
  font-size: 14px;
  font-weight: 500;
  cursor: pointer;
  transition: all 0.2s;
}

.add-btn:hover {
  background: #2563eb;
  border-color: #2563eb;
}

.recording-sets-list {
  display: flex;
  flex-direction: column;
  gap: 12px;
}

.recording-set-item {
  padding: 16px;
  border: 1px solid #e5e7eb;
  border-radius: 12px;
  cursor: pointer;
  transition: all 0.2s;
}

.recording-set-item:hover {
  border-color: #3b82f6;
  background: #f8fafc;
}

.recording-set-item.active {
  border-color: #3b82f6;
  background: #eff6ff;
}

.set-info h4 {
  font-size: 16px;
  font-weight: 600;
  color: #1f2937;
  margin: 0 0 4px 0;
}

.set-info p {
  font-size: 14px;
  color: #6b7280;
  margin: 0 0 8px 0;
}

.progress-bar {
  width: 100%;
  height: 4px;
  background: #e5e7eb;
  border-radius: 2px;
  overflow: hidden;
}

.progress-fill {
  height: 100%;
  background: #3b82f6;
  transition: width 0.3s ease;
}

/* Options Modal */
.option-item {
  display: flex;
  align-items: center;
  padding: 16px;
  border-radius: 12px;
  cursor: pointer;
  transition: all 0.2s;
  margin-bottom: 4px;
}

.option-item:hover {
  background: #f3f4f6;
}

.option-icon {
  font-size: 20px;
  margin-right: 12px;
  width: 24px;
  text-align: center;
}

.option-text {
  font-size: 16px;
  color: #374151;
  font-weight: 500;
}

.speed-cycle {
  border-color: #f59e0b;
  color: #f59e0b;
}

.speed-cycle:hover:not(:disabled) {
  background: #fef3c7;
  border-color: #d97706;
}

.vad-toggle {
  border-color: #8b5cf6;
  color: #8b5cf6;
}

.vad-toggle:hover:not(:disabled) {
  background: #f3f4f6;
  border-color: #7c3aed;
}

.vad-toggle.vad-enabled {
  background: #8b5cf6;
  color: white;
  border-color: #7c3aed;
}

.vad-toggle.vad-enabled:hover:not(:disabled) {
  background: #7c3aed;
  border-color: #6d28d9;
}

.vad-toggle.vad-processing {
  background: #f59e0b;
  color: white;
  border-color: #d97706;
  animation: vadProcessing 1.5s infinite;
}

.align-btn {
  border-color: #06b6d4;
  color: #06b6d4;
}

.align-btn:hover:not(:disabled) {
  background: #ecfeff;
  border-color: #0891b2;
}

@keyframes vadProcessing {
  0%, 100% { opacity: 1; }
  50% { opacity: 0.7; }
}

/* Alignment Status Indicator */
.alignment-indicator {
  display: flex;
  align-items: center;
  justify-content: center;
  gap: 8px;
  padding: 8px 12px;
  margin: 8px 16px;
  background: linear-gradient(135deg, #10b981 0%, #047857 100%);
  color: white;
  border-radius: 8px;
  font-size: 12px;
  font-weight: 500;
  box-shadow: 0 2px 4px rgba(16, 185, 129, 0.2);
  animation: alignmentGlow 2s ease-in-out;
}

.alignment-icon {
  font-size: 14px;
}

.alignment-text {
  font-size: 11px;
  letter-spacing: 0.3px;
}

@keyframes alignmentGlow {
  0% { 
    box-shadow: 0 2px 4px rgba(16, 185, 129, 0.2);
  }
  50% { 
    box-shadow: 0 4px 12px rgba(16, 185, 129, 0.4);
    transform: translateY(-1px);
  }
  100% { 
    box-shadow: 0 2px 4px rgba(16, 185, 129, 0.2);
    transform: translateY(0);
  }
}

/* Recording Status Section */
.recording-status-section {
  padding: 12px 16px;
  background: #fef2f2;
  border: 1px solid #fecaca;
  border-radius: 8px;
  margin: 0 16px;
}

.recording-indicator {
  display: flex;
  flex-direction: column;
  align-items: center;
  gap: 4px;
}

.recording-indicator .status-text {
  font-size: 14px;
  color: #dc2626;
  font-weight: 600;
}

.recording-indicator .recording-timer {
  font-size: 16px;
  color: #ef4444;
  font-weight: 700;
  font-family: monospace;
}

.audio-controls {
  display: flex;
  align-items: center;
  gap: 12px;
  flex-wrap: wrap;
}

.speed-controls {
  display: flex;
  gap: 4px;
  margin-left: auto;
}

.play-btn {
  width: 44px;
  height: 44px;
  border: none;
  border-radius: 50%;
  background: rgba(255, 255, 255, 0.2);
  color: white;
  font-size: 20px;
  cursor: pointer;
  transition: all 0.2s;
}

.play-btn:hover {
  background: rgba(255, 255, 255, 0.3);
}

.duration {
  font-size: 14px;
  font-weight: 500;
}

/* Waveform & Spectrogram */
.waveform-container,
.spectrogram-container {
  border-bottom: 1px solid #e5e7eb;
}

.waveform-header,
.spectrogram-header {
  display: flex;
  align-items: center;
  justify-content: space-between;
  padding: 12px 16px;
  background: #f9fafb;
  border-bottom: 1px solid #e5e7eb;
}

.waveform-header h3,
.spectrogram-header h3 {
  font-size: 14px;
  font-weight: 600;
  color: #374151;
  margin: 0;
  letter-spacing: 0.5px;
}

.waveform-controls {
  display: flex;
  gap: 4px;
}

.speed-btn {
  padding: 4px 8px;
  border: 1px solid #d1d5db;
  border-radius: 4px;
  background: white;
  font-size: 12px;
  color: #374151;
  cursor: pointer;
}

.speed-btn:hover {
  background: #f3f4f6;
}

.expand-btn {
  background: none;
  border: none;
  font-size: 16px;
  cursor: pointer;
  padding: 4px;
}

/* Working visualization structure from AudioPlayer */
.visualization-wrapper {
  background-color: #1a1a1a;
  width: 100%;
  display: flex;
  flex-direction: column;
  border-radius: 8px;
  overflow: hidden;
}

.waveform-container {
  width: 100%;
  height: 40px;
  /* Compressed height for mobile */
  background-color: rgba(0, 0, 0, 0.2);
  /* Semi-transparent background */
  border: 1px solid #60a5fa;
  /* Reduced border width */
  flex-shrink: 0;
  /* Don't shrink */
  box-sizing: border-box;
  /* Include border in height calculation */
  position: relative;
  /* For absolute positioning of placeholders */
}

/* Ensure WaveSurfer elements are properly styled */
.waveform-container>* {
  width: 100%;
}

/* Style placeholders */
.placeholder-wave,
.placeholder-spectrogram {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 100%;
  height: 100%;
  font-size: 14px;
  color: #6b7280;
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1;
}

.spectrogram-container {
  width: 100%;
  height: 120px;
  /* Compressed height for mobile */
  flex-shrink: 0;
  /* Don't shrink */
  position: relative;
  /* For absolute positioning of placeholders */
}

/* Style placeholders */
.placeholder-wave,
.placeholder-spectrogram {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 100%;
  height: 100%;
  font-size: 14px;
  color: #6b7280;
}

/* Recording Section */
.recording-section {
  padding: 24px 16px;
  text-align: center;
}

.recording-status {
  margin-bottom: 16px;
}

.status-text {
  font-size: 16px;
  font-weight: 500;
  color: #374151;
  display: block;
}

.recording-timer {
  font-size: 14px;
  color: #ef4444;
  font-weight: 600;
  margin-top: 4px;
  display: block;
}

.record-button {
  width: 120px;
  height: 120px;
  border: none;
  border-radius: 50%;
  background: linear-gradient(135deg, #3b82f6 0%, #1d4ed8 100%);
  color: white;
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  transition: all 0.3s;
  box-shadow: 0 8px 25px rgba(59, 130, 246, 0.3);
  margin: 0 auto 24px;
}

.record-button.recording {
  background: linear-gradient(135deg, #ef4444 0%, #dc2626 100%);
  animation: pulse 2s infinite;
  box-shadow: 0 8px 25px rgba(239, 68, 68, 0.4);
}

.record-button.processing {
  background: linear-gradient(135deg, #f59e0b 0%, #d97706 100%);
  box-shadow: 0 8px 25px rgba(245, 158, 11, 0.3);
}

@keyframes pulse {

  0%,
  100% {
    transform: scale(1);
  }

  50% {
    transform: scale(1.05);
  }
}

.record-icon {
  font-size: 32px;
  margin-bottom: 4px;
}

.record-text {
  font-size: 12px;
  font-weight: 600;
  text-transform: uppercase;
  letter-spacing: 0.5px;
}

.quick-settings {
  display: flex;
  justify-content: center;
  gap: 24px;
  flex-wrap: wrap;
}

.setting-toggle {
  display: flex;
  align-items: center;
  gap: 8px;
  font-size: 14px;
  color: #374151;
  cursor: pointer;
}

.setting-toggle input {
  margin: 0;
}

/* User Audio Section Actions */
.recording-actions {
  display: flex;
  gap: 8px;
}

.action-btn {
  padding: 6px 12px;
  border: 1px solid #d1d5db;
  border-radius: 6px;
  background: white;
  font-size: 12px;
  color: #374151;
  cursor: pointer;
  transition: all 0.2s;
}

.action-btn:hover {
  background: #f3f4f6;
}

/* Comparison Section */
.comparison-section {
  padding: 16px;
}

.comparison-header h2 {
  font-size: 16px;
  font-weight: 600;
  color: #374151;
  margin: 0 0 12px 0;
}

.comparison-controls {
  display: flex;
  gap: 8px;
  flex-wrap: wrap;
}

.comparison-btn {
  flex: 1;
  min-width: 100px;
  padding: 12px;
  border: 1px solid #d1d5db;
  border-radius: 8px;
  background: white;
  font-size: 13px;
  color: #374151;
  cursor: pointer;
  transition: all 0.2s;
}

.comparison-btn:hover {
  background: #f3f4f6;
  border-color: #9ca3af;
}

/* Action Panel */
.action-panel {
  padding: 20px 16px;
  background: white;
  border-top: 1px solid #e5e7eb;
}

.action-buttons {
  display: flex;
  gap: 12px;
  margin-bottom: 16px;
}

.action-primary {
  flex: 1;
  padding: 16px;
  border: none;
  border-radius: 12px;
  font-size: 16px;
  font-weight: 600;
  cursor: pointer;
  transition: all 0.2s;
}

.save-btn {
  background: linear-gradient(135deg, #3b82f6 0%, #1d4ed8 100%);
  color: white;
}

.save-btn:hover:not(:disabled) {
  transform: translateY(-1px);
  box-shadow: 0 4px 12px rgba(59, 130, 246, 0.3);
}

.complete-btn {
  background: linear-gradient(135deg, #10b981 0%, #047857 100%);
  color: white;
}

.complete-btn:hover:not(:disabled) {
  transform: translateY(-1px);
  box-shadow: 0 4px 12px rgba(16, 185, 129, 0.3);
}

.action-primary:disabled {
  background: #9ca3af;
  cursor: not-allowed;
  transform: none;
  box-shadow: none;
}

.navigation-buttons {
  display: flex;
  gap: 12px;
}

.nav-btn {
  flex: 1;
  padding: 12px;
  border: 1px solid #d1d5db;
  border-radius: 8px;
  background: white;
  color: #374151;
  font-size: 14px;
  font-weight: 500;
  cursor: pointer;
  transition: all 0.2s;
}

.nav-btn:hover {
  background: #f3f4f6;
  border-color: #9ca3af;
}

/* Bottom Safe Area */
.bottom-safe-area {
  height: env(safe-area-inset-bottom);
  background: white;
}

/* Responsive adjustments */
@media (max-width: 375px) {
  .loading-controls {
    flex-direction: column;
  }

  .quick-settings {
    flex-direction: column;
    align-items: center;
  }

  .comparison-controls {
    flex-direction: column;
  }

  .action-buttons {
    flex-direction: column;
  }
}

@media (min-width: 768px) {
  .mobile-practice-a {
    max-width: 480px;
    margin: 0 auto;
    box-shadow: 0 0 20px rgba(0, 0, 0, 0.1);
  }
}

/* URL Input Modal */
.url-input {
  width: 100%;
  padding: 12px;
  border: 1px solid #d1d5db;
  border-radius: 8px;
  font-size: 14px;
  margin-bottom: 16px;
  box-sizing: border-box;
}

.url-input:focus {
  outline: none;
  border-color: #3b82f6;
  box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.1);
}

.modal-actions {
  display: flex;
  justify-content: flex-end;
  gap: 8px;
}

.cancel-btn, .load-btn-modal {
  padding: 8px 16px;
  border-radius: 6px;
  font-size: 14px;
  font-weight: 500;
  cursor: pointer;
  transition: all 0.2s;
}

.cancel-btn {
  border: 1px solid #d1d5db;
  background: white;
  color: #374151;
}

.cancel-btn:hover {
  background: #f3f4f6;
}

.load-btn-modal {
  border: 1px solid #3b82f6;
  background: #3b82f6;
  color: white;
}

.load-btn-modal:hover:not(:disabled) {
  background: #2563eb;
}

.load-btn-modal:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}
</style>
