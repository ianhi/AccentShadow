<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Simple VAD Test</title>
    <style>
        body { font-family: Arial, sans-serif; padding: 20px; max-width: 800px; margin: 0 auto; }
        .result { background: #f0f0f0; padding: 10px; margin: 10px 0; font-family: monospace; white-space: pre-wrap; }
        button { padding: 10px 20px; margin: 5px; cursor: pointer; }
        .success { background: #d4edda; }
        .error { background: #f8d7da; }
        .warning { background: #fff3cd; }
        .audio-section {
            margin: 20px 0;
            padding: 15px;
            border: 1px solid #ddd;
            border-radius: 8px;
            background: #f9f9f9;
        }
        #waveform, #spectrogram {
            margin: 10px 0;
            border: 1px solid #ccc;
            border-radius: 4px;
            position: relative;
        }
        .vad-marker {
            position: absolute;
            top: 0;
            bottom: 0;
            border-left: 2px solid #ff0000;
            pointer-events: none;
            z-index: 10;
        }
        .vad-marker.start {
            border-left: 3px solid #00ff00;
        }
        .vad-marker.end {
            border-left: 3px solid #ff0000;
        }
        .vad-region {
            position: absolute;
            top: 0;
            bottom: 0;
            background: rgba(255, 255, 0, 0.2);
            pointer-events: none;
            z-index: 5;
            border: 1px solid rgba(255, 255, 0, 0.6);
        }
        .marker-label {
            position: absolute;
            top: -20px;
            font-size: 10px;
            color: #333;
            background: rgba(255, 255, 255, 0.8);
            padding: 1px 3px;
            border-radius: 2px;
            white-space: nowrap;
        }
    </style>
    <!-- Load ONNX runtime first, then VAD -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.14.0/dist/ort.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.19/dist/bundle.min.js"></script>
    <script src="https://unpkg.com/wavesurfer.js@7"></script>
</head>
<body>
    <h1>🎤 Simple VAD Test</h1>
    <p>Testing if VAD can detect ANY speech in our audio files</p>
    
    <button onclick="testMinimalVAD()">Test Minimal VAD</button>
    <button onclick="testWithDifferentModel()">Test v5 Model</button>
    <button onclick="generateTestTone()">Generate & Test Tone</button>
    <button onclick="visualizeAudio()">Visualize Audio</button>
    <button onclick="debugSampleRates()">Debug Sample Rates</button>
    <button onclick="testVADWithoutResampling()">Test VAD on Original (No Resample)</button>
    <button onclick="testMinimalProcessing()">Test Minimal Processing</button>
    <button onclick="testSimplifiedVAD()">Test Simplified VAD (Documentation Pattern)</button>
    
    <div class="audio-section">
        <h3>Audio Visualization</h3>
        <audio id="audio-player" controls style="width: 100%; margin-bottom: 10px;"></audio>
        
        <h4>Waveform:</h4>
        <div id="waveform" style="height: 80px; background: #f0f0f0;"></div>
        
        <h4>Spectrogram with VAD Markers:</h4>
        <div style="font-size: 12px; color: #666; margin-bottom: 5px;">
            🟢 Green lines = Speech start | 🔴 Red lines = Speech end | 🟡 Yellow regions = Detected speech
        </div>
        <div id="spectrogram" style="height: 200px; background: #f0f0f0;"></div>
        
        <h4>VAD Timeline:</h4>
        <div id="vad-overlay" style="position: relative; height: 20px; background: rgba(0,0,0,0.1); margin-top: 5px;"></div>
    </div>
    
    <div id="results"></div>

    <script>
        let wavesurfer = null;
        let currentAudioBuffer = null;
        let currentVADSegments = [];
        
        function addResult(text, type = '') {
            const div = document.createElement('div');
            div.className = `result ${type}`;
            div.textContent = text;
            document.getElementById('results').appendChild(div);
        }
        
        async function visualizeAudio() {
            addResult('Loading and visualizing patth.wav...');
            
            try {
                // Load audio
                const response = await fetch('/patth.wav');
                const blob = await response.blob();
                const audioUrl = URL.createObjectURL(blob);
                
                // Set up audio player
                document.getElementById('audio-player').src = audioUrl;
                
                // Decode audio for analysis
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const arrayBuffer = await blob.arrayBuffer();
                currentAudioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                
                // Initialize WaveSurfer
                if (wavesurfer) {
                    wavesurfer.destroy();
                }
                
                // Import WaveSurfer plugins
                const { default: WaveSurfer } = await import('https://unpkg.com/wavesurfer.js@7/dist/wavesurfer.esm.js');
                const { default: Spectrogram } = await import('https://unpkg.com/wavesurfer.js@7/dist/plugins/spectrogram.esm.js');
                
                wavesurfer = WaveSurfer.create({
                    container: '#waveform',
                    waveColor: '#4F4A85',
                    progressColor: '#383351',
                    height: 80,
                    normalize: true,
                    barWidth: 2,
                    barRadius: 3
                });
                
                // Add spectrogram
                const spectrogramPlugin = Spectrogram.create({
                    container: '#spectrogram',
                    labels: true,
                    height: 200,
                    splitChannels: false,
                    fftSamples: 512,
                    windowFunc: 'hann'
                });
                
                wavesurfer.registerPlugin(spectrogramPlugin);
                wavesurfer.load(audioUrl);
                
                wavesurfer.on('ready', () => {
                    addResult(`Visualization ready: ${currentAudioBuffer.duration.toFixed(3)}s`, 'success');
                    drawVADOverlay();
                    drawSpectrogramMarkers();
                });
                
                // Analyze amplitude
                analyzeAmplitude();
                
            } catch (error) {
                addResult(`Visualization error: ${error.message}`, 'error');
            }
        }
        
        function analyzeAmplitude() {
            if (!currentAudioBuffer) return;
            
            const channelData = currentAudioBuffer.getChannelData(0);
            const windowSize = Math.floor(currentAudioBuffer.sampleRate * 0.01); // 10ms windows
            
            let maxAmp = 0, avgAmp = 0, nonZeroSamples = 0;
            const amplitudes = [];
            
            // Calculate windowed amplitudes
            for (let i = 0; i < channelData.length; i += windowSize) {
                const windowEnd = Math.min(i + windowSize, channelData.length);
                let windowSum = 0;
                
                for (let j = i; j < windowEnd; j++) {
                    const sample = Math.abs(channelData[j]);
                    windowSum += sample;
                    if (sample > 0.001) nonZeroSamples++;
                    maxAmp = Math.max(maxAmp, sample);
                }
                
                amplitudes.push(windowSum / (windowEnd - i));
            }
            
            avgAmp = amplitudes.reduce((a, b) => a + b, 0) / amplitudes.length;
            
            addResult(`Audio Analysis:
Max amplitude: ${maxAmp.toFixed(6)}
Avg amplitude: ${avgAmp.toFixed(6)}
Non-zero samples: ${nonZeroSamples}/${channelData.length} (${(nonZeroSamples/channelData.length*100).toFixed(1)}%)
Audio level: ${maxAmp > 0.1 ? 'Good' : maxAmp > 0.01 ? 'Low' : 'Very Low'}`, 'success');
        }
        
        function drawVADOverlay() {
            const overlay = document.getElementById('vad-overlay');
            overlay.innerHTML = '';
            
            if (currentVADSegments.length === 0) {
                overlay.innerHTML = '<div style="text-align: center; line-height: 20px; color: #999;">No VAD segments to display</div>';
                return;
            }
            
            const duration = currentAudioBuffer.duration;
            
            currentVADSegments.forEach((segment, index) => {
                const startPercent = (segment.startTime / duration) * 100;
                const widthPercent = ((segment.endTime - segment.startTime) / duration) * 100;
                
                const segmentDiv = document.createElement('div');
                segmentDiv.style.cssText = `
                    position: absolute;
                    left: ${startPercent}%;
                    width: ${widthPercent}%;
                    height: 100%;
                    background: rgba(255, 0, 0, 0.6);
                    border: 1px solid red;
                    box-sizing: border-box;
                `;
                segmentDiv.title = `Segment ${index + 1}: ${segment.startTime.toFixed(3)}s - ${segment.endTime.toFixed(3)}s`;
                overlay.appendChild(segmentDiv);
            });
        }
        
        function drawSpectrogramMarkers() {
            if (!currentAudioBuffer || currentVADSegments.length === 0) return;
            
            const spectrogramContainer = document.getElementById('spectrogram');
            const duration = currentAudioBuffer.duration;
            
            // Clear existing markers
            spectrogramContainer.querySelectorAll('.vad-marker, .vad-region, .marker-label').forEach(el => el.remove());
            
            currentVADSegments.forEach((segment, index) => {
                const startPercent = (segment.startTime / duration) * 100;
                const endPercent = (segment.endTime / duration) * 100;
                const widthPercent = endPercent - startPercent;
                
                // Create background region
                const region = document.createElement('div');
                region.className = 'vad-region';
                region.style.left = startPercent + '%';
                region.style.width = widthPercent + '%';
                region.title = `VAD Segment ${index + 1}: ${segment.startTime.toFixed(3)}s - ${segment.endTime.toFixed(3)}s`;
                spectrogramContainer.appendChild(region);
                
                // Create start marker
                const startMarker = document.createElement('div');
                startMarker.className = 'vad-marker start';
                startMarker.style.left = startPercent + '%';
                spectrogramContainer.appendChild(startMarker);
                
                // Create start label
                const startLabel = document.createElement('div');
                startLabel.className = 'marker-label';
                startLabel.style.left = startPercent + '%';
                startLabel.textContent = `${segment.startTime.toFixed(3)}s`;
                spectrogramContainer.appendChild(startLabel);
                
                // Create end marker
                const endMarker = document.createElement('div');
                endMarker.className = 'vad-marker end';
                endMarker.style.left = endPercent + '%';
                spectrogramContainer.appendChild(endMarker);
                
                // Create end label
                const endLabel = document.createElement('div');
                endLabel.className = 'marker-label';
                endLabel.style.left = endPercent + '%';
                endLabel.style.top = '-35px'; // Offset to avoid overlap with start label
                endLabel.textContent = `${segment.endTime.toFixed(3)}s`;
                spectrogramContainer.appendChild(endLabel);
            });
            
            addResult(`Added ${currentVADSegments.length} VAD markers to spectrogram`, 'success');
        }

        async function testMinimalVAD() {
            addResult('Testing with minimal VAD configuration...');
            
            try {
                // Check for ONNX runtime first
                if (!window.ort) {
                    addResult('ONNX runtime not available!', 'error');
                    addResult('This is required for VAD to work.', 'error');
                    return;
                }
                
                // Wait for VAD
                if (!window.vad) {
                    addResult('Waiting for VAD library...', 'warning');
                    let retries = 0;
                    while (!window.vad && retries < 50) {
                        await new Promise(resolve => setTimeout(resolve, 100));
                        retries++;
                    }
                }
                
                if (!window.vad) {
                    addResult('VAD library not available after waiting!', 'error');
                    return;
                }
                
                addResult(`ONNX runtime version: ${window.ort.version || 'unknown'}`, 'success');
                addResult(`VAD library loaded: ${Object.keys(window.vad).join(', ')}`, 'success');
                
                // Create ultra-sensitive VAD to catch all speech
                addResult('Creating VAD with ultra-sensitive settings to catch all speech...');
                const vadInstance = await window.vad.NonRealTimeVAD.new({
                    positiveSpeechThreshold: 0.005, // Ultra-sensitive
                    negativeSpeechThreshold: 0.001, // Almost zero
                    redemptionFrames: 512,          // Massive gap allowance
                    frameSamples: 1536,
                    minSpeechFrames: 1,             // Minimum possible
                    preSpeechPadFrames: 64,         // Lots of padding
                    positiveSpeechPadFrames: 64     // Lots of padding
                });
                
                addResult('VAD instance created successfully', 'success');
                
                // Load patth.wav
                addResult('\nLoading patth.wav...');
                const response = await fetch('/patth.wav');
                const blob = await response.blob();
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const arrayBuffer = await blob.arrayBuffer();
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                
                addResult(`Audio loaded: ${audioBuffer.duration.toFixed(3)}s, ${audioBuffer.sampleRate}Hz`, 'success');
                
                // Check if audio has content
                const channelData = audioBuffer.getChannelData(0);
                let maxAmp = 0;
                for (let i = 0; i < channelData.length; i++) {
                    maxAmp = Math.max(maxAmp, Math.abs(channelData[i]));
                }
                addResult(`Max amplitude in audio: ${maxAmp.toFixed(6)}`);
                
                // Resample to 16kHz
                addResult('\nResampling to 16kHz for VAD...');
                let processBuffer = audioBuffer;
                if (audioBuffer.sampleRate !== 16000) {
                    const offlineContext = new OfflineAudioContext(1, Math.ceil(audioBuffer.duration * 16000), 16000);
                    const bufferSource = offlineContext.createBufferSource();
                    bufferSource.buffer = audioBuffer;
                    bufferSource.connect(offlineContext.destination);
                    bufferSource.start();
                    processBuffer = await offlineContext.startRendering();
                }
                
                const audioData = processBuffer.getChannelData(0);
                addResult(`Resampled to ${audioData.length} samples`);
                
                // Run VAD
                addResult('\nRunning VAD detection...');
                addResult(`Audio for VAD: ${audioData.length} samples at ${processBuffer.sampleRate}Hz`);
                addResult(`Expected duration: ${(audioData.length / processBuffer.sampleRate).toFixed(3)}s`);
                
                let segmentCount = 0;
                currentVADSegments = []; // Store for visualization
                
                for await (const segment of vadInstance.run(audioData, processBuffer.sampleRate)) {
                    segmentCount++;
                    const startTime = segment.start / processBuffer.sampleRate;
                    const endTime = segment.end / processBuffer.sampleRate;
                    const duration = endTime - startTime;
                    
                    addResult(`RAW Segment ${segmentCount}: start=${segment.start} end=${segment.end} samples`);
                    addResult(`CONVERTED Segment ${segmentCount}: ${startTime.toFixed(3)}s - ${endTime.toFixed(3)}s (${(duration * 1000).toFixed(0)}ms)`);
                    
                    currentVADSegments.push({ startTime, endTime });
                }
                
                if (segmentCount === 0) {
                    addResult('\n❌ NO SEGMENTS DETECTED even with extreme sensitivity!', 'error');
                    addResult('This suggests a fundamental issue with VAD or audio format.', 'error');
                } else {
                    addResult(`\n✅ Total segments detected: ${segmentCount}`, 'success');
                    
                    // Auto-visualize if we have segments
                    if (segmentCount > 0) {
                        setTimeout(() => {
                            addResult('\nAuto-loading visualization...', 'warning');
                            visualizeAudio();
                        }, 1000);
                    }
                    
                    // If visualization already exists, update markers
                    if (wavesurfer && currentAudioBuffer) {
                        drawSpectrogramMarkers();
                    }
                }
                
            } catch (error) {
                addResult(`ERROR: ${error.message}`, 'error');
                console.error('Full error:', error);
            }
        }

        async function testWithDifferentModel() {
            addResult('\nTesting with v5 model instead of v4...');
            
            try {
                if (!window.vad) {
                    addResult('VAD library not available!', 'error');
                    return;
                }
                
                // Try to use v5 model
                addResult('Creating VAD with v5 model...');
                const vadInstance = await window.vad.NonRealTimeVAD.new({
                    positiveSpeechThreshold: 0.3,
                    negativeSpeechThreshold: 0.15,
                    redemptionFrames: 32,
                    frameSamples: 512,  // Different frame size for v5
                    minSpeechFrames: 2,
                    preSpeechPadFrames: 8,
                    positiveSpeechPadFrames: 8,
                    // Try to force v5 model if available
                    modelURL: 'https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.19/dist/silero_vad_v5.onnx'
                });
                
                // Test with patth.wav
                const response = await fetch('/patth.wav');
                const blob = await response.blob();
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const arrayBuffer = await blob.arrayBuffer();
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                
                // Resample to 16kHz
                let processBuffer = audioBuffer;
                if (audioBuffer.sampleRate !== 16000) {
                    const offlineContext = new OfflineAudioContext(1, Math.ceil(audioBuffer.duration * 16000), 16000);
                    const bufferSource = offlineContext.createBufferSource();
                    bufferSource.buffer = audioBuffer;
                    bufferSource.connect(offlineContext.destination);
                    bufferSource.start();
                    processBuffer = await offlineContext.startRendering();
                }
                
                const audioData = processBuffer.getChannelData(0);
                
                let segmentCount = 0;
                for await (const segment of vadInstance.run(audioData, processBuffer.sampleRate)) {
                    segmentCount++;
                    const startTime = segment.start / processBuffer.sampleRate;
                    const endTime = segment.end / processBuffer.sampleRate;
                    addResult(`Segment ${segmentCount}: ${startTime.toFixed(3)}s - ${endTime.toFixed(3)}s`, 'success');
                }
                
                if (segmentCount === 0) {
                    addResult('❌ No segments with v5 model either', 'error');
                } else {
                    addResult(`✅ v5 model detected ${segmentCount} segments!`, 'success');
                }
                
            } catch (error) {
                addResult(`v5 test error: ${error.message}`, 'error');
            }
        }

        async function generateTestTone() {
            addResult('\nGenerating test tone to verify VAD works at all...');
            
            try {
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const sampleRate = 16000;
                const duration = 2; // 2 seconds
                const frequency = 440; // A4 note
                
                // Create buffer with sine wave
                const buffer = audioContext.createBuffer(1, sampleRate * duration, sampleRate);
                const data = buffer.getChannelData(0);
                
                for (let i = 0; i < data.length; i++) {
                    // Generate sine wave with envelope
                    const t = i / sampleRate;
                    const envelope = t < 0.5 ? 0 : t > 1.5 ? 0 : 1; // Silent at start/end
                    data[i] = Math.sin(2 * Math.PI * frequency * t) * 0.5 * envelope;
                }
                
                addResult('Generated 440Hz tone with 0.5s silence at start and end');
                
                // Test with VAD
                const vadInstance = await window.vad.NonRealTimeVAD.new({
                    positiveSpeechThreshold: 0.3,
                    negativeSpeechThreshold: 0.15,
                    redemptionFrames: 24,
                    frameSamples: 1536,
                    minSpeechFrames: 4,
                    preSpeechPadFrames: 4,
                    positiveSpeechPadFrames: 4
                });
                
                const audioData = buffer.getChannelData(0);
                let segmentCount = 0;
                
                for await (const segment of vadInstance.run(audioData, sampleRate)) {
                    segmentCount++;
                    const startTime = segment.start / sampleRate;
                    const endTime = segment.end / sampleRate;
                    addResult(`Tone segment ${segmentCount}: ${startTime.toFixed(3)}s - ${endTime.toFixed(3)}s`, 'success');
                }
                
                if (segmentCount === 0) {
                    addResult('❌ VAD cannot even detect a pure tone!', 'error');
                    addResult('This indicates a fundamental VAD issue.', 'error');
                } else {
                    addResult('✅ VAD can detect synthetic audio', 'success');
                    addResult('Problem might be specific to our audio files', 'warning');
                }
                
            } catch (error) {
                addResult(`Test tone error: ${error.message}`, 'error');
            }
        }
        
        async function debugSampleRates() {
            addResult('=== SAMPLE RATE DEBUG ===');
            
            try {
                // Load patth.wav
                const response = await fetch('/patth.wav');
                const blob = await response.blob();
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const arrayBuffer = await blob.arrayBuffer();
                const originalBuffer = await audioContext.decodeAudioData(arrayBuffer);
                
                addResult(`Original Audio:
Sample Rate: ${originalBuffer.sampleRate}Hz
Duration: ${originalBuffer.duration.toFixed(3)}s
Length: ${originalBuffer.length} samples`);
                
                // Test resampling to 16kHz
                if (originalBuffer.sampleRate !== 16000) {
                    addResult('\nResampling to 16kHz...');
                    
                    const offlineContext = new OfflineAudioContext(
                        1, // mono
                        Math.ceil(originalBuffer.duration * 16000),
                        16000
                    );
                    
                    const bufferSource = offlineContext.createBufferSource();
                    bufferSource.buffer = originalBuffer;
                    bufferSource.connect(offlineContext.destination);
                    bufferSource.start();
                    
                    const resampledBuffer = await offlineContext.startRendering();
                    
                    addResult(`Resampled Audio:
Sample Rate: ${resampledBuffer.sampleRate}Hz
Duration: ${resampledBuffer.duration.toFixed(3)}s
Length: ${resampledBuffer.length} samples
Duration Match: ${Math.abs(originalBuffer.duration - resampledBuffer.duration) < 0.001 ? '✅ Good' : '❌ BAD'}`);
                    
                    // Check if content shifted during resampling
                    const originalData = originalBuffer.getChannelData(0);
                    const resampledData = resampledBuffer.getChannelData(0);
                    
                    // Find peak in original
                    let originalPeakIndex = 0;
                    let originalPeakValue = 0;
                    for (let i = 0; i < originalData.length; i++) {
                        if (Math.abs(originalData[i]) > originalPeakValue) {
                            originalPeakValue = Math.abs(originalData[i]);
                            originalPeakIndex = i;
                        }
                    }
                    const originalPeakTime = originalPeakIndex / originalBuffer.sampleRate;
                    
                    // Find peak in resampled
                    let resampledPeakIndex = 0;
                    let resampledPeakValue = 0;
                    for (let i = 0; i < resampledData.length; i++) {
                        if (Math.abs(resampledData[i]) > resampledPeakValue) {
                            resampledPeakValue = Math.abs(resampledData[i]);
                            resampledPeakIndex = i;
                        }
                    }
                    const resampledPeakTime = resampledPeakIndex / resampledBuffer.sampleRate;
                    
                    addResult(`Peak Analysis:
Original peak at: ${originalPeakTime.toFixed(3)}s (sample ${originalPeakIndex})
Resampled peak at: ${resampledPeakTime.toFixed(3)}s (sample ${resampledPeakIndex})
Time shift: ${Math.abs(originalPeakTime - resampledPeakTime).toFixed(3)}s
Shift acceptable: ${Math.abs(originalPeakTime - resampledPeakTime) < 0.01 ? '✅ Good' : '❌ BAD - Content shifted!'}`);
                    
                } else {
                    addResult('Audio is already at 16kHz - no resampling needed');
                }
                
            } catch (error) {
                addResult(`Debug error: ${error.message}`, 'error');
            }
        }
        
        async function testVADWithoutResampling() {
            addResult('=== TESTING VAD ON ORIGINAL AUDIO (NO RESAMPLING) ===');
            
            try {
                if (!window.vad) {
                    addResult('VAD not available!', 'error');
                    return;
                }
                
                // Load original audio
                const response = await fetch('/patth.wav');
                const blob = await response.blob();
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const arrayBuffer = await blob.arrayBuffer();
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                
                addResult(`Testing VAD on original audio:
Sample Rate: ${audioBuffer.sampleRate}Hz
Duration: ${audioBuffer.duration.toFixed(3)}s`);
                
                // Create VAD instance that works with the original sample rate
                const vadInstance = await window.vad.NonRealTimeVAD.new({
                    positiveSpeechThreshold: 0.001, // Even more sensitive
                    negativeSpeechThreshold: 0.0001,
                    redemptionFrames: 1024,
                    frameSamples: 1536,
                    minSpeechFrames: 1,
                    preSpeechPadFrames: 128,
                    positiveSpeechPadFrames: 128
                });
                
                // Use original audio data without resampling
                const audioData = audioBuffer.getChannelData(0);
                addResult(`Using original audio: ${audioData.length} samples`);
                
                let segmentCount = 0;
                const originalSegments = [];
                
                // Try VAD on original sample rate
                try {
                    for await (const segment of vadInstance.run(audioData, audioBuffer.sampleRate)) {
                        segmentCount++;
                        const startTime = segment.start / audioBuffer.sampleRate;
                        const endTime = segment.end / audioBuffer.sampleRate;
                        
                        addResult(`ORIGINAL RATE Segment ${segmentCount}: ${startTime.toFixed(3)}s - ${endTime.toFixed(3)}s`);
                        originalSegments.push({ startTime, endTime });
                    }
                } catch (vadError) {
                    addResult(`VAD failed on original sample rate: ${vadError.message}`, 'error');
                    addResult('This might explain why we need resampling to 16kHz', 'warning');
                }
                
                if (segmentCount === 0) {
                    addResult('No segments detected even on original audio', 'warning');
                } else {
                    addResult(`Found ${segmentCount} segments on original sample rate`, 'success');
                    
                    // Compare with what we expect from the spectrogram
                    addResult('\nExpected from spectrogram: Large speech content in middle section');
                    originalSegments.forEach((seg, i) => {
                        const duration = seg.endTime - seg.startTime;
                        addResult(`Segment ${i+1}: ${(duration*1000).toFixed(0)}ms - ${duration > 0.5 ? '✅ Reasonable' : '⚠️ Very short'}`);
                    });
                }
                
            } catch (error) {
                addResult(`Error: ${error.message}`, 'error');
            }
        }
        
        async function testMinimalProcessing() {
            addResult('=== TESTING MINIMAL PROCESSING - LET VAD HANDLE EVERYTHING ===');
            
            try {
                if (!window.vad) {
                    addResult('VAD not available!', 'error');
                    return;
                }
                
                // Load original audio
                const response = await fetch('/patth.wav');
                const blob = await response.blob();
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const arrayBuffer = await blob.arrayBuffer();
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                
                addResult(`Original Audio Properties:
Sample Rate: ${audioBuffer.sampleRate}Hz
Duration: ${audioBuffer.duration.toFixed(3)}s
Channels: ${audioBuffer.numberOfChannels}
Expected main speech: ~0.5s to ~1.2s (from spectrogram visualization)`);
                
                const originalData = audioBuffer.getChannelData(0);
                
                // Test 1: Let VAD handle sample rate internally (most important test)
                addResult('\n--- Test 1: Pass original sample rate to VAD (Let VAD handle internally) ---');
                
                try {
                    const vadInstance1 = await window.vad.NonRealTimeVAD.new({
                        positiveSpeechThreshold: 0.3,  // Moderate sensitivity
                        negativeSpeechThreshold: 0.15,
                        redemptionFrames: 24,
                        frameSamples: 1536,            // Keep standard frame size
                        minSpeechFrames: 4,
                        preSpeechPadFrames: 8,
                        positiveSpeechPadFrames: 8
                    });
                    
                    let segmentCount1 = 0;
                    const segments1 = [];
                    
                    // Pass original sample rate - let VAD handle it internally
                    for await (const segment of vadInstance1.run(originalData, audioBuffer.sampleRate)) {
                        segmentCount1++;
                        const startTime = segment.start / audioBuffer.sampleRate;
                        const endTime = segment.end / audioBuffer.sampleRate;
                        segments1.push({ startTime, endTime });
                        addResult(`Original SR Segment ${segmentCount1}: ${startTime.toFixed(3)}s - ${endTime.toFixed(3)}s (${((endTime - startTime) * 1000).toFixed(0)}ms)`);
                    }
                    
                    if (segmentCount1 === 0) {
                        addResult(`❌ VAD accepted original sample rate but found NO segments`);
                    } else {
                        addResult(`✅ VAD accepted original sample rate! Found ${segmentCount1} segments`);
                        
                        // Check if any segment overlaps with expected speech region (0.5s - 1.2s)
                        const expectedStart = 0.5;
                        const expectedEnd = 1.2;
                        let foundExpectedSpeech = false;
                        
                        for (const seg of segments1) {
                            if (seg.startTime < expectedEnd && seg.endTime > expectedStart) {
                                foundExpectedSpeech = true;
                                addResult(`✅ Found segment overlapping expected speech region: ${seg.startTime.toFixed(3)}s - ${seg.endTime.toFixed(3)}s`);
                            }
                        }
                        
                        if (!foundExpectedSpeech) {
                            addResult(`⚠️ WARNING: No segments found in expected speech region (0.5s - 1.2s)`);
                        }
                    }
                    
                } catch (vadError1) {
                    addResult(`❌ VAD rejected original sample rate: ${vadError1.message}`);
                    addResult('This means VAD REQUIRES 16kHz resampling internally');
                }
                
                // Test 2: Compare with our manual resampling approach
                addResult('\n--- Test 2: Manual resampling to 16kHz (current approach) ---');
                
                try {
                    const vadInstance2 = await window.vad.NonRealTimeVAD.new({
                        positiveSpeechThreshold: 0.3,
                        negativeSpeechThreshold: 0.15,
                        redemptionFrames: 24,
                        frameSamples: 1536,
                        minSpeechFrames: 4,
                        preSpeechPadFrames: 8,
                        positiveSpeechPadFrames: 8
                    });
                    
                    // Manual resampling (current approach)
                    let resampledBuffer = audioBuffer;
                    if (audioBuffer.sampleRate !== 16000) {
                        addResult(`Manually resampling from ${audioBuffer.sampleRate}Hz to 16kHz...`);
                        const offlineContext = new OfflineAudioContext(1, Math.ceil(audioBuffer.duration * 16000), 16000);
                        const bufferSource = offlineContext.createBufferSource();
                        bufferSource.buffer = audioBuffer;
                        bufferSource.connect(offlineContext.destination);
                        bufferSource.start();
                        resampledBuffer = await offlineContext.startRendering();
                    }
                    
                    const resampledData = resampledBuffer.getChannelData(0);
                    let segmentCount2 = 0;
                    const segments2 = [];
                    
                    for await (const segment of vadInstance2.run(resampledData, 16000)) {
                        segmentCount2++;
                        const startTime = segment.start / 16000;  // Use 16kHz for time calculation
                        const endTime = segment.end / 16000;
                        segments2.push({ startTime, endTime });
                        addResult(`Manual Resample Segment ${segmentCount2}: ${startTime.toFixed(3)}s - ${endTime.toFixed(3)}s (${((endTime - startTime) * 1000).toFixed(0)}ms)`);
                    }
                    
                    addResult(`Manual resampling found ${segmentCount2} segments`);
                    
                } catch (vadError2) {
                    addResult(`❌ Manual resampling approach failed: ${vadError2.message}`);
                }
                
                // Test 3: Force 16kHz with original data (lie about sample rate)
                addResult('\n--- Test 3: Lie about sample rate (tell VAD original data is 16kHz) ---');
                
                try {
                    const vadInstance3 = await window.vad.NonRealTimeVAD.new({
                        positiveSpeechThreshold: 0.3,
                        negativeSpeechThreshold: 0.15,
                        redemptionFrames: 24,
                        frameSamples: 1536,
                        minSpeechFrames: 4,
                        preSpeechPadFrames: 8,
                        positiveSpeechPadFrames: 8
                    });
                    
                    let segmentCount3 = 0;
                    // Lie about sample rate - tell VAD original data is 16kHz when it's not
                    for await (const segment of vadInstance3.run(originalData, 16000)) {
                        segmentCount3++;
                        // Calculate times using the LIE (16kHz) - this will show timing distortion
                        const fakeStartTime = segment.start / 16000;
                        const fakeEndTime = segment.end / 16000;
                        
                        // Calculate ACTUAL times using true sample rate
                        const actualStartTime = segment.start / audioBuffer.sampleRate;
                        const actualEndTime = segment.end / audioBuffer.sampleRate;
                        
                        addResult(`Fake 16kHz Segment ${segmentCount3}:`);
                        addResult(`  Fake times: ${fakeStartTime.toFixed(3)}s - ${fakeEndTime.toFixed(3)}s`);
                        addResult(`  Real times: ${actualStartTime.toFixed(3)}s - ${actualEndTime.toFixed(3)}s`);
                        addResult(`  Time distortion: ${((actualStartTime - fakeStartTime) * 1000).toFixed(0)}ms`);
                    }
                    
                    if (segmentCount3 > 0) {
                        addResult(`⚠️ VAD processed with fake 16kHz! Found ${segmentCount3} segments`);
                        addResult('This proves that lying about sample rate causes timing errors!');
                    } else {
                        addResult(`❌ No segments even with fake 16kHz`);
                    }
                    
                } catch (vadError3) {
                    addResult(`❌ Fake 16kHz approach failed: ${vadError3.message}`);
                }
                
                // Test 4: Ultra-sensitive with original sample rate
                addResult('\n--- Test 4: Ultra-sensitive with original sample rate ---');
                
                try {
                    const vadInstance4 = await window.vad.NonRealTimeVAD.new({
                        positiveSpeechThreshold: 0.05,  // Ultra-sensitive
                        negativeSpeechThreshold: 0.01,
                        redemptionFrames: 128,
                        frameSamples: 1536,
                        minSpeechFrames: 1,
                        preSpeechPadFrames: 32,
                        positiveSpeechPadFrames: 32
                    });
                    
                    let segmentCount4 = 0;
                    for await (const segment of vadInstance4.run(originalData, audioBuffer.sampleRate)) {
                        segmentCount4++;
                        const startTime = segment.start / audioBuffer.sampleRate;
                        const endTime = segment.end / audioBuffer.sampleRate;
                        addResult(`Ultra-Sensitive Segment ${segmentCount4}: ${startTime.toFixed(3)}s - ${endTime.toFixed(3)}s`);
                    }
                    
                    addResult(`Ultra-sensitive found ${segmentCount4} segments`);
                    
                } catch (vadError4) {
                    addResult(`❌ Ultra-sensitive failed: ${vadError4.message}`);
                }
                
                // Conclusion
                addResult('\n=== MINIMAL PROCESSING CONCLUSION ===');
                addResult('Key findings:');
                addResult('1. Can VAD handle original sample rates? (Test 1 result above)');
                addResult('2. Does manual resampling work? (Test 2 result above)');
                addResult('3. Does lying about sample rate cause timing errors? (Test 3 result above)');
                addResult('4. Does sensitivity help with original sample rates? (Test 4 result above)');
                
            } catch (error) {
                addResult(`Minimal processing error: ${error.message}`, 'error');
            }
        }

        async function testSimplifiedVAD() {
            addResult('=== TESTING SIMPLIFIED VAD (FOLLOWING DOCUMENTATION PATTERN) ===');
            
            try {
                if (!window.vad) {
                    addResult('VAD not available!', 'error');
                    return;
                }
                
                // Load original audio
                const response = await fetch('/patth.wav');
                const blob = await response.blob();
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const arrayBuffer = await blob.arrayBuffer();
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                
                addResult(`Audio Properties:
Sample Rate: ${audioBuffer.sampleRate}Hz (native)
Duration: ${audioBuffer.duration.toFixed(3)}s
Expected speech region: 0.5s - 1.2s (from visual analysis)`);
                
                // Follow the exact documentation pattern
                addResult('\n--- Following Official Documentation Pattern ---');
                addResult('const myvad = await vad.NonRealTimeVAD.new()');
                addResult('for await (const {audio, start, end} of myvad.run(audioFileData, nativeSampleRate))');
                addResult('// start and end are in MILLISECONDS');
                
                // Create VAD instance with minimal config (like documentation)
                const myvad = await window.vad.NonRealTimeVAD.new();
                addResult('✅ VAD instance created with default settings');
                
                // Get audio data and native sample rate (exactly as in docs)
                const audioFileData = audioBuffer.getChannelData(0);
                const nativeSampleRate = audioBuffer.sampleRate;
                
                addResult(`\nProcessing ${audioFileData.length} samples at native ${nativeSampleRate}Hz...`);
                
                let segmentCount = 0;
                const segments = [];
                
                // Use the exact pattern from documentation
                for await (const {audio, start, end} of myvad.run(audioFileData, nativeSampleRate)) {
                    segmentCount++;
                    
                    // start and end are in MILLISECONDS according to docs
                    const startSeconds = start / 1000;
                    const endSeconds = end / 1000;
                    const durationMs = end - start;
                    
                    segments.push({ startSeconds, endSeconds, startMs: start, endMs: end });
                    
                    addResult(`Segment ${segmentCount}:`);
                    addResult(`  Raw values: start=${start}ms, end=${end}ms`);
                    addResult(`  Time: ${startSeconds.toFixed(3)}s - ${endSeconds.toFixed(3)}s`);
                    addResult(`  Duration: ${durationMs}ms`);
                    addResult(`  Audio samples: ${audio ? audio.length : 'N/A'}`);
                    
                    // Check if this overlaps with expected speech region (0.5s - 1.2s)
                    if (startSeconds < 1.2 && endSeconds > 0.5) {
                        addResult(`  ✅ OVERLAPS with expected speech region!`);
                    } else {
                        addResult(`  ⚠️ Outside expected speech region (0.5s - 1.2s)`);
                    }
                }
                
                addResult(`\n=== SIMPLIFIED VAD RESULTS ===`);
                if (segmentCount === 0) {
                    addResult(`❌ NO SPEECH SEGMENTS DETECTED`, 'error');
                    addResult('Even the simplified approach found no speech', 'error');
                } else {
                    addResult(`✅ Found ${segmentCount} speech segments using simplified approach`, 'success');
                    
                    // Check if any segment is in the expected region
                    const foundExpectedSpeech = segments.some(seg => 
                        seg.startSeconds < 1.2 && seg.endSeconds > 0.5
                    );
                    
                    if (foundExpectedSpeech) {
                        addResult(`✅ SUCCESS: Found speech in expected region (0.5s - 1.2s)`, 'success');
                        
                        // Update visualization if available
                        currentVADSegments = segments.map(seg => ({
                            startTime: seg.startSeconds,
                            endTime: seg.endSeconds
                        }));
                        
                        if (wavesurfer && currentAudioBuffer) {
                            drawSpectrogramMarkers();
                        }
                        
                        // Create trimmed audio preview
                        await createTrimmedAudioPreview(segments);
                        
                    } else {
                        addResult(`⚠️ WARNING: No segments in expected speech region`, 'warning');
                    }
                    
                    // Show timing comparison
                    addResult('\nTiming Analysis:');
                    const earliestStart = Math.min(...segments.map(s => s.startSeconds));
                    const latestEnd = Math.max(...segments.map(s => s.endSeconds));
                    addResult(`Overall speech span: ${earliestStart.toFixed(3)}s - ${latestEnd.toFixed(3)}s`);
                    addResult(`Expected span: 0.500s - 1.200s`);
                    addResult(`Timing accuracy: ${Math.abs(earliestStart - 0.5) < 0.1 && Math.abs(latestEnd - 1.2) < 0.3 ? 'GOOD' : 'NEEDS REVIEW'}`);
                }
                
            } catch (error) {
                addResult(`Simplified VAD error: ${error.message}`, 'error');
                console.error('Simplified VAD error details:', error);
            }
        }

        // Add audio trimming preview functionality
        async function createTrimmedAudioPreview(segments) {
            if (!currentAudioBuffer || segments.length === 0) return;
            
            try {
                addResult('\n--- Creating Trimmed Audio Preview ---');
                
                // Calculate overall boundaries with padding
                const earliestStart = Math.min(...segments.map(s => s.startSeconds));
                const latestEnd = Math.max(...segments.map(s => s.endSeconds));
                const padding = 0.1; // 100ms padding
                
                const trimStart = Math.max(0, earliestStart - padding);
                const trimEnd = Math.min(currentAudioBuffer.duration, latestEnd + padding);
                
                addResult(`Original duration: ${currentAudioBuffer.duration.toFixed(3)}s`);
                addResult(`Speech detected: ${earliestStart.toFixed(3)}s - ${latestEnd.toFixed(3)}s`);
                addResult(`With padding: ${trimStart.toFixed(3)}s - ${trimEnd.toFixed(3)}s`);
                addResult(`Trimmed duration: ${(trimEnd - trimStart).toFixed(3)}s`);
                
                // Create trimmed audio buffer
                const sampleRate = currentAudioBuffer.sampleRate;
                const startSample = Math.floor(trimStart * sampleRate);
                const endSample = Math.floor(trimEnd * sampleRate);
                const trimmedLength = endSample - startSample;
                
                const trimmedBuffer = new AudioContext().createBuffer(
                    currentAudioBuffer.numberOfChannels,
                    trimmedLength,
                    sampleRate
                );
                
                // Copy trimmed audio data
                for (let channel = 0; channel < currentAudioBuffer.numberOfChannels; channel++) {
                    const originalData = currentAudioBuffer.getChannelData(channel);
                    const trimmedData = trimmedBuffer.getChannelData(channel);
                    
                    for (let i = 0; i < trimmedLength; i++) {
                        const sourceIndex = startSample + i;
                        trimmedData[i] = sourceIndex < originalData.length ? originalData[sourceIndex] : 0;
                    }
                }
                
                // Convert to blob and create preview
                const trimmedBlob = await audioBufferToBlob(trimmedBuffer);
                const trimmedUrl = URL.createObjectURL(trimmedBlob);
                
                // Create audio preview element
                const previewContainer = document.createElement('div');
                previewContainer.innerHTML = `
                    <h4>🎵 Trimmed Audio Preview</h4>
                    <p>This is what the audio would sound like after VAD trimming:</p>
                    <audio controls style="width: 100%; margin: 10px 0;">
                        <source src="${trimmedUrl}" type="audio/wav">
                    </audio>
                    <p><strong>Reduction:</strong> ${((currentAudioBuffer.duration - (trimEnd - trimStart)) / currentAudioBuffer.duration * 100).toFixed(1)}% shorter</p>
                `;
                
                // Add to audio section
                const audioSection = document.querySelector('.audio-section');
                
                // Remove existing preview if any
                const existingPreview = audioSection.querySelector('.trimmed-preview');
                if (existingPreview) {
                    existingPreview.remove();
                }
                
                previewContainer.className = 'trimmed-preview';
                previewContainer.style.cssText = `
                    margin: 20px 0;
                    padding: 15px;
                    background: #e8f5e8;
                    border: 2px solid #28a745;
                    border-radius: 8px;
                `;
                audioSection.appendChild(previewContainer);
                
                addResult('✅ Trimmed audio preview created!', 'success');
                
            } catch (error) {
                addResult(`Error creating audio preview: ${error.message}`, 'error');
            }
        }

        // Convert AudioBuffer to WAV Blob
        async function audioBufferToBlob(audioBuffer) {
            const numberOfChannels = audioBuffer.numberOfChannels;
            const sampleRate = audioBuffer.sampleRate;
            const length = audioBuffer.length;
            const bytesPerSample = 2; // 16-bit PCM
            const blockAlign = numberOfChannels * bytesPerSample;
            const byteRate = sampleRate * blockAlign;
            const dataSize = length * blockAlign;
            const bufferSize = 44 + dataSize;
            
            const arrayBuffer = new ArrayBuffer(bufferSize);
            const view = new DataView(arrayBuffer);
            
            // Write WAV header
            const writeString = (offset, string) => {
                for (let i = 0; i < string.length; i++) {
                    view.setUint8(offset + i, string.charCodeAt(i));
                }
            };
            
            writeString(0, 'RIFF');
            view.setUint32(4, bufferSize - 8, true);
            writeString(8, 'WAVE');
            writeString(12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true);
            view.setUint16(22, numberOfChannels, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, byteRate, true);
            view.setUint16(32, blockAlign, true);
            view.setUint16(34, 16, true);
            writeString(36, 'data');
            view.setUint32(40, dataSize, true);
            
            // Convert and write audio samples
            let offset = 44;
            for (let i = 0; i < length; i++) {
                for (let channel = 0; channel < numberOfChannels; channel++) {
                    const sample = Math.max(-1, Math.min(1, audioBuffer.getChannelData(channel)[i]));
                    view.setInt16(offset, sample * 0x7FFF, true);
                    offset += 2;
                }
            }
            
            return new Blob([arrayBuffer], { type: 'audio/wav' });
        }

        // Make functions available globally
        window.testMinimalVAD = testMinimalVAD;
        window.testWithDifferentModel = testWithDifferentModel;
        window.generateTestTone = generateTestTone;
        window.visualizeAudio = visualizeAudio;
        window.debugSampleRates = debugSampleRates;
        window.testVADWithoutResampling = testVADWithoutResampling;
        window.testMinimalProcessing = testMinimalProcessing;
        window.testSimplifiedVAD = testSimplifiedVAD;

        // Auto-run simplified VAD test on page load
        document.addEventListener('DOMContentLoaded', async () => {
            addResult('🎙️ Simple VAD Test Page Ready', 'success');
            addResult('Auto-loading audio and running simplified VAD documentation pattern...', 'info');
            
            // Wait for libraries to load, then auto-run
            setTimeout(async () => {
                try {
                    // Load visualization first
                    await visualizeAudio();
                    
                    // Wait for waveform to be ready, then run simplified VAD
                    setTimeout(async () => {
                        await testSimplifiedVAD();
                    }, 2000);
                } catch (error) {
                    addResult(`Auto-run error: ${error.message}`, 'error');
                }
            }, 1000);
        });
    </script>
</body>
</html>