<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VAD Processing Test</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        .test-section {
            margin: 20px 0;
            padding: 15px;
            border: 1px solid #ddd;
            border-radius: 8px;
        }
        .test-result {
            background: #f5f5f5;
            padding: 10px;
            margin: 10px 0;
            border-radius: 4px;
            font-family: monospace;
            white-space: pre-wrap;
        }
        .success { background: #d4edda; border-left: 4px solid #28a745; }
        .error { background: #f8d7da; border-left: 4px solid #dc3545; }
        .warning { background: #fff3cd; border-left: 4px solid #ffc107; }
        button {
            background: #007bff;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 4px;
            cursor: pointer;
            margin: 5px;
        }
        button:hover { background: #0056b3; }
        button:disabled { background: #6c757d; cursor: not-allowed; }
        .progress {
            background: #e9ecef;
            border-radius: 4px;
            height: 20px;
            margin: 10px 0;
        }
        .progress-bar {
            background: #007bff;
            height: 100%;
            border-radius: 4px;
            transition: width 0.3s;
        }
    </style>
    <script src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.19/dist/bundle.min.js"></script>
</head>
<body>
    <h1>üéôÔ∏è VAD Processing Test</h1>
    <p>Testing the restored VAD configuration from commit 3f94da2</p>

    <div class="test-section">
        <h2>üìÅ Test Audio Files</h2>
        <button onclick="testFile('path.mp3')">Test path.mp3</button>
        <button onclick="testFile('patth.wav')">Test patth.wav</button>
        <button onclick="testFile('test_said_three_words.wav')">Test test_said_three_words.wav</button>
        <button onclick="testAllFiles()">Test All Files</button>
        
        <div id="progress-container" style="display: none;">
            <div class="progress">
                <div id="progress-bar" class="progress-bar" style="width: 0%"></div>
            </div>
            <p id="progress-text">Initializing...</p>
        </div>
    </div>

    <div class="test-section">
        <h2>üîß VAD Configuration Test</h2>
        <div class="test-result" id="config-test">
            Balanced Configuration (improved from 3f94da2):
            - positiveSpeechThreshold: 0.4 (more sensitive)
            - negativeSpeechThreshold: 0.25 (better continuation)
            - redemptionFrames: 24
            - minSpeechFrames: 6 (reduced for better detection)
            - padding: 100ms
        </div>
    </div>

    <div id="test-results"></div>

    <script type="module">
        // Import VAD processor functionality (simplified for testing)
        let vadInstance = null;
        let vadReady = false;

        const VAD_CONFIG = {
            positiveSpeechThreshold: 0.4,  // More balanced for better silence detection
            negativeSpeechThreshold: 0.25, // Lower threshold for better speech continuation
            redemptionFrames: 24,
            frameSamples: 1536,
            minSpeechFrames: 6,             // Reduced for better detection
            preSpeechPadFrames: 4,
            positiveSpeechPadFrames: 4
        };

        async function initVAD() {
            try {
                addResult('üîß Initializing VAD with working configuration...', 'info');
                
                // Wait for VAD library
                let retries = 0;
                while (!window.vad && retries < 50) {
                    await new Promise(resolve => setTimeout(resolve, 100));
                    retries++;
                }
                
                if (!window.vad) {
                    throw new Error('VAD library not loaded');
                }
                
                addResult('üì¶ Creating VAD instance with conservative settings...', 'info');
                vadInstance = await window.vad.NonRealTimeVAD.new(VAD_CONFIG);
                vadReady = true;
                
                addResult('‚úÖ VAD initialized successfully with working configuration', 'success');
                return true;
            } catch (error) {
                addResult(`‚ùå VAD initialization failed: ${error.message}`, 'error');
                return false;
            }
        }

        async function testFile(filename) {
            if (!vadReady) {
                const initialized = await initVAD();
                if (!initialized) return;
            }

            showProgress(true);
            updateProgress(0, `Loading ${filename}...`);

            try {
                // Load audio file
                const response = await fetch(`/${filename}`);
                if (!response.ok) {
                    throw new Error(`Failed to load ${filename}: ${response.status}`);
                }
                
                const audioBlob = await response.blob();
                updateProgress(20, `Loaded ${filename} (${audioBlob.size} bytes)`);

                // Get original duration
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                const arrayBuffer = await audioBlob.arrayBuffer();
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                const originalDuration = audioBuffer.duration;

                updateProgress(40, 'Analyzing with VAD...');

                // Test VAD detection
                const vadResult = await detectSpeechWithVAD(audioBlob, audioBuffer);
                updateProgress(80, 'Processing results...');

                // Calculate reduction
                const speechDuration = vadResult.endTime - vadResult.startTime;
                const reductionPercent = ((originalDuration - speechDuration) / originalDuration) * 100;

                updateProgress(100, 'Complete!');

                // Display results
                addResult(`üéôÔ∏è ${filename} Results:`, 'info');
                addResult(`Original: ${originalDuration.toFixed(3)}s | Speech: ${speechDuration.toFixed(3)}s | Reduction: ${reductionPercent.toFixed(1)}%`, 'info');
                addResult(`Speech segments: ${vadResult.segments} | Confidence: ${(vadResult.confidence * 100).toFixed(1)}%`, 'info');
                addResult(`Start: ${vadResult.startTime.toFixed(3)}s | End: ${vadResult.endTime.toFixed(3)}s`, 'info');

                // Validate results
                if (reductionPercent > 50) {
                    addResult(`‚ö†Ô∏è WARNING: Audio reduced by ${reductionPercent.toFixed(1)}% - may be over-trimming`, 'warning');
                } else if (vadResult.segments === 0) {
                    addResult(`‚ùå ERROR: No speech segments detected`, 'error');
                } else {
                    addResult(`‚úÖ SUCCESS: Speech detected properly, reasonable reduction`, 'success');
                }

            } catch (error) {
                addResult(`‚ùå Error testing ${filename}: ${error.message}`, 'error');
            } finally {
                showProgress(false);
            }
        }

        async function detectSpeechWithVAD(audioBlob, audioBuffer) {
            // Resample to 16kHz for VAD
            const targetSampleRate = 16000;
            let resampledBuffer = audioBuffer;
            
            if (audioBuffer.sampleRate !== targetSampleRate) {
                const offlineContext = new OfflineAudioContext(
                    1, // mono
                    Math.ceil(audioBuffer.duration * targetSampleRate),
                    targetSampleRate
                );
                
                const bufferSource = offlineContext.createBufferSource();
                bufferSource.buffer = audioBuffer;
                bufferSource.connect(offlineContext.destination);
                bufferSource.start();
                
                resampledBuffer = await offlineContext.startRendering();
            }

            // Get audio data for VAD
            const audioData = resampledBuffer.getChannelData(0);
            
            // Run VAD detection
            const speechSegments = [];
            for await (const { audio, start, end } of vadInstance.run(audioData, resampledBuffer.sampleRate)) {
                const startTimeSeconds = start / resampledBuffer.sampleRate;
                const endTimeSeconds = end / resampledBuffer.sampleRate;
                
                speechSegments.push({
                    startTime: startTimeSeconds,
                    endTime: endTimeSeconds,
                    audioLength: audio.length
                });
            }

            // Calculate overall boundaries
            let overallStart = null;
            let overallEnd = null;
            let confidence = 0;

            if (speechSegments.length > 0) {
                overallStart = Math.min(...speechSegments.map(s => s.startTime));
                overallEnd = Math.max(...speechSegments.map(s => s.endTime));
                
                // Calculate confidence based on speech coverage
                const totalSpeechDuration = speechSegments.reduce((sum, seg) => sum + (seg.endTime - seg.startTime), 0);
                confidence = Math.min(1, totalSpeechDuration / (resampledBuffer.duration * 0.8));
                
                // Apply padding
                const padding = 0.1; // 100ms
                overallStart = Math.max(0, overallStart - padding);
                overallEnd = Math.min(resampledBuffer.duration, overallEnd + padding);
            } else {
                // No speech detected - use original boundaries
                overallStart = 0;
                overallEnd = audioBuffer.duration;
                confidence = 0;
            }

            return {
                startTime: overallStart,
                endTime: overallEnd,
                segments: speechSegments.length,
                confidence: confidence
            };
        }

        async function testAllFiles() {
            const files = ['path.mp3', 'patth.wav', 'test_said_three_words.wav'];
            addResult('üîÑ Testing all files...', 'info');
            
            for (const file of files) {
                await testFile(file);
                await new Promise(resolve => setTimeout(resolve, 1000)); // Brief pause between tests
            }
            
            addResult('‚úÖ All file tests completed', 'success');
        }

        function addResult(message, type = 'info') {
            const resultsDiv = document.getElementById('test-results');
            const resultDiv = document.createElement('div');
            resultDiv.className = `test-result ${type}`;
            resultDiv.textContent = `${new Date().toLocaleTimeString()}: ${message}`;
            resultsDiv.appendChild(resultDiv);
            resultsDiv.scrollTop = resultsDiv.scrollHeight;
        }

        function showProgress(show) {
            document.getElementById('progress-container').style.display = show ? 'block' : 'none';
        }

        function updateProgress(percent, text) {
            document.getElementById('progress-bar').style.width = percent + '%';
            document.getElementById('progress-text').textContent = text;
        }

        // Make functions available globally
        window.testFile = testFile;
        window.testAllFiles = testAllFiles;

        // Initialize on page load
        document.addEventListener('DOMContentLoaded', () => {
            addResult('üéôÔ∏è VAD Processing Test Page Ready', 'success');
            addResult('Using working configuration from commit 3f94da2', 'info');
        });
    </script>
</body>
</html>